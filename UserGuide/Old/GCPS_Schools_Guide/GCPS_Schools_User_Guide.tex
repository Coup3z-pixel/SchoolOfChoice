\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{amsthm}
\makeatletter
\usepackage{graphicx,epsf}
\usepackage{times,float}
\usepackage{enumerate}
\usepackage[round,comma]{natbib}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}
\usepackage{bm}
\usepackage{multirow}
%\usepackage{blkarray}
\usepackage{rotating}

\setlength{\textwidth}{6.4in} \setlength{\textheight}{8.5in}
\setlength{\topmargin}{-.2in} \setlength{\oddsidemargin}{.1in}
\renewcommand{\baselinestretch}{1.3}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem*{thm*}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem*{lem*}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{rem}{Remark}
\newtheorem{ex}{Example}
\newtheorem{fact}{Fact}
\newtheorem*{fact*}{Fact}
\newtheorem{remark}{Remark}


\newcommand{\rR}{\mathrel{R}}
\newcommand{\rP}{\mathrel{P}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\norev}{\medskip \centerline{\textbf{No Revisions Below}} \medskip}
\renewcommand{\Re}{\mathbb{R}}
\newcommand{\In}{\mathbb{Z}}

\newcommand{\bare}{\overline{e}}
\newcommand{\bark}{{\overline k}}
\newcommand{\barl}{\overline{l}}
\newcommand{\barp}{\overline{p}}
\newcommand{\bart}{{\overline t}}

\newcommand{\bq}{\mathbf{q}}

\newcommand{\cE}{\mathcal{E}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cX}{\mathcal{X}}

\newcommand{\dr}{{\dot r}}
\newcommand{\dq}{{\dot q}}
\newcommand{\dg}{{\dot g}}
\newcommand{\ddp}{{\dot p}}

\newcommand{\hA}{{\hat A}}
\newcommand{\hO}{{\hat O}}

\newcommand{\halpha}{{\hat \alpha}}

\newcommand{\ta}{{\tilde a}}
\newcommand{\te}{{\tilde e}}
\newcommand{\tn}{{\tilde n}}

\newcommand{\tB}{{\tilde B}}
\newcommand{\tP}{{\tilde P}}

\newcommand{\varep}{\varepsilon}

\newcommand{\bone}{\mathbf{1}}

\begin{document}

\title{GCPS Schools: A User's Guide}

\author{Andrew McLennan\footnote{School of Economics, University of
    Queensland, {\tt a.mclennan@economics.uq.edu.au}}}

\date{\today}

\maketitle

\begin{abstract}
This document provides a brief introduction to the software package GCPS Schools.
\end{abstract}

% \pagebreak

\section{Introduction}

The software package GCPS Schools provides the software tools that are
required to use the GCPS and MCC mechanisms, which are new school choice
mechanisms, with several advantages in comparison with the current
leading alternatives.  The software in this package has been
extensively tested, it is reliable, it is fast enough to be used even
in very large school choice problems.  It is ready for real world
application.

The academic paper ``Efficient Computationally Tractable School Choice
Mechanisms'' (joint with Shino Takayama and Yuki Tamura) describes the
GCPS and MCC mechanisms in formal mathematical detail.  It also
briefly describes the history of school choice, other mechanisms that
have been proposed and are in use, and related academic literature.
This document describes the software from the point of view of a user,
and from the point of view of a programmer.  It does not assume that
you have read the other paper, and is in this sense self contained.
Nevertheless the reader will certainly have a much better
understanding of why this topic is interesting and what this software
is good for if he or she also reads as much of the other paper as his
or her appetite for mathematical formality permits.

\subsection{Better School Choice Mechanisms}

From time immemorial until about 25 or 30 years ago, each school had a
district, and each student was required to attend the school whose
district contained her residence, unless she enrolled in a private
school.  This has various problems such as school segregation
reproducing residential segregation, but the main issue for us is that
it forbids students from trading their assignments in ways that are
mutually beneficial.

School choice schemes allow students to attend public schools that
they express a preference for.  In the schemes discussed here, each
student is assigned a \emph{safe school}, to which she is guaranteed
to be admitted if no other school admits her.  Each student submits a
rank ordered list of the schools that she likes at least as much as
her safe school, with the safe school at the bottom.  Each school has
a complete transitive ordering of the students, which is called a
\emph{priority}.  A \emph{school choice mechanism} is an algorithm
that takes the preferences and the priorities as inputs and outputs an
assignment of each student to one of the schools she ranked that is
feasible, in the sense that the number of students assigned to each
school is not greater than its capacity.  Such an algorithm is a
\emph{mechanism}.

This document describes software that implements new and better
algorithms for school choice.  In the remainder of this introduction
we give an overview of the components of this software package, and
explain why it improves on the methods used previously.  The next
section describes the software from the point of view of the user: how
to download and set up the software, the information the user needs to
supply, the formats of input and output files, how to invoke the
commands that process the files, and so forth.  The final, longest
section describes the software from the point of view of a programmer
who wishes to understand some part of the code, possibly with an eye to
modifying it to meet her needs.

\subsection{The GCPS Mechanism}

The GCPS mechanism is appropriate when the schools' priorities are
\emph{dichotomous}: for each school and student, the student is either
eligible to attend the school or she is not.  Eligibility may be
affected by gender, residential location, or test scores in the case
of selective schools, and a student is also eligible only if the school
is one the ones she ranked.

\norev

\subsection{Why GCPS and MCC are Better}

We nwo describe other mechanisms that have been used to school choice,
and how GCPS and MCC imporove on these.  One of the first methods,
called the \emph{Boston mechanism} or \emph{immediate acceptance},
requires each student to submit a ranking of the schools.  The
mechanism first assigns as many students as possible to their top
ranked schools, then assigns as many of the remaining students to the
schools they ranked second, and so forth. A big problem with the
Boston mechanism is that being honest about your preferences can be
disadvantageous.  For example, suppose there are three high schools,
called Harvard, Yale, and Cornell.  Harvard and Yale each have 200
seats in their entering class, and Cornell has 600 seats.  Almost
everyone prefers Harvard to Yale and Yale to Cornell, and if all
students state their preferences truthfully, then each has a 20\%
chance of going to Harvard, a 20\% chance of going to Yale, and a 60\%
chance of going to Cornell.  If everyone else is truthful, and you
list Yale as your top choice, then you go there for sure.  But
everyone can see this problem, and many people will not be truthful,
so there is a quite tricky game in which you have to anticipate the
extent to which others are trying to manipulate the mechanism.

The \emph{student proposes deferred acceptance} (DA) mechanism was
first proposed in the academic literature by Gale and Shapley, but
later people realized that it had already been used very successfully
for several years to match medical school graduates with residencies.
In the first round of deferred acceptance each student applies to her
favorite school.  Each school with more applicants than seats
tentatively accepts its favorite applicants up to its capacity and
rejects all the others.  In the second round each student who was
rejected in the first round applies to her second favorite school, and
each school retains its favorite applicants from those who applied in
both rounds, up to its capacity, and rejects the others.  In each
subsequent round each student who was rejected in the preceeding round
applies to her favorite school among those that have not yet rejected
her, and each school hangs on to its favorite applicants, from all
rounds, up to its capacity, and rejects all others.  This continues
until there is a round with no rejections, at which point the existing
tentative acceptances become the final match.

An assignment of each student to some school is \emph{feasible} if the
number of students assigned to each school is not greater than the
school's capacity.  A feasible assignment is \emph{stable} if there is
no student-school pair $(i,o)$ such that the $i$ prefers $o$ to the
school she has been assigned to and $o$ either has an empty seat or
prefers $i$ to some other student that has been assigned to $o$.  The
match produced by DA is stable: if $i$ prefers $o$ to the school she
has been assigned to, $o$ must have rejected $i$ at some stage, and
$o$'s pool of applicants only expanded after that.

Consider a particular school $o$.  In any stable match, if a student
for whom $o$ is the favorite is not matched with $o$, $o$ cannot be
matched to any student it likes less than $i$.  In particular,
students that $o$ rejects in the first round of DA are not matched to
$o$ in any stable match.  It follows that in any stable match, if a
student $i$ who applies to $o$ in the second round is not matched with
$o$, $o$ cannot be matched to any student it likes less than $i$.
Similarly, students who apply to $o$ in the third round are not
matched to the schools they like better in any stable matching, so if
they are rejected by $o$, either immediately or later, it must be the
case that in any stable matching $o$ is matched only to students it
likes better.  Continuing this logic inductively, we find that any
student who is rejected by $o$ during the DA process is not matched to
$o$ in any stable match.  That is, in any stable match the students
who are matched with $o$ are at least as good as $o$'s favorite
students from the ones that apply to $o$ in DA, and in this sense the
match produced by DA is the worst for $o$ among all stable matches.

For each student $i$, the schools that reject $i$ during DA are all
schools that are not matched with $i$ in any stable matching, so the
school that $i$ is matched with under DA is the best school for $i$ in
any stable matching.  Now consider what happens if $i$ tries to
manipulate the process by reporting a preference ordering of the
schools that is different from her true preference.  In the DA
procedure applying to a school and getting rejected has the same
effect on the process as not applying, so nothing changes before we
get to the best school $o$ that might accept $i$, and $i$ can only do
worse by saying that she doesn't like $o$ when she actually does.  For
the students, honesty is the best policy.  The technical terminology
is that DA is \emph{strategy proof} or \emph{nonmanipulable} for the
students.  It turns out that DA is not strategy proof for the
schools\footnote{In the setting originally considered by Gale and
Shapley, with boys proposing to girls, if Alice is holding a proposal
from Harry and receives a proposal from Bob, who she prefers to Harry,
she might nevertheless do best to reject Bob if the result is that Bob
proposes to Carol, who then dumps David, after which David proposes to
Alice, which is what Alice really wanted all along.}.

The main reason for presenting all this theory, about a mechanism that
isn't even one of the ones the software implements, is to explain why
students, parents, and school administrators find DA extremely
confusing.  In particular, students and parents do not understand that
DA is strategy proof for the students, and experimental studies find
that misreporting of preferences is quite common.  Largely for these
reasons, the Boston mechanism is still in widespread use around the
world, in spite of its clear cut theoretical inferiority.

From the point of view of a student $i$, the GCPS and MCC mechanisms
allocate as much probability of $i$ receiving a seat in her favorite
school as possible, after which they allocate as much probability as
possible of $i$ receiving a seat in her favorite school among those
that still have some unassigned capacity, and so forth.  In the GCPS
mechanism this happens over the unit interval of time: each student
``consumes'' probability of her favorite school, at unit speed, until
some school (or group of schools) can no longer assign probability to
students who have other alternatives.  In the MCC mechanism each
school has a cutoff quality level, students below that level receive
no probability of a seat at that school, students above that level
receive as much probability as they like, and students at the cutoff
level can receive some probability of a seat, but may be rationed.
There is some mathematical magic under the hood, but at the top level
the main thrust of the GCPS and MCC mechanisms is that the student is
given what she says she wants, to the extent possible, and this will
be what she actually wants if what she says she wants is, in fact,
what she does want.  So long as the student expects (correctly) that
her declaration of preferences will not have a large effect on times
at which schools restrict access in the GCPS mechanism or the cutoffs
of the MCC mechanism, she has a strong incentive to reveal her
preferences truthfully.

DA requires that each school has a strict ranking of all students,
which is called a \emph{priority}.  In some school systems each school
should, in principle, give equal consideration to all eligible
students.  In such cases the priorities are arbitrary, and usually
chosen at random.  This can give rise to inefficiencies.  For example,
if Bob likes Carol School and Ted likes Alice School, the mechanism
may still match Bob with Alice School and Ted with Carol School if
Carol School ``prefers'' Ted and Alice School ``prefers'' Bob.  Longer
cycles of potentially improving trades are also possible.  Such
inefficiencies have been found to be quantitatively important in
practice.  Applying the GCPS mechanism instead of DA eliminates such
inefficiencies.

In some school systems there are ``coarse'' priorities that reflect
actual social values.  A common example is that, among eligible
students, those with a sibling at the school who live in the school's
walk zone have highest priority, those with a sibling at the school
who live outside the walk zone have second priority, those without a
sibling at the school who live in the walk zone have third priority,
and other eligible students have lowest priority.  In order to apply
DA there must be (usually randomly generated) strict pirorities that
refine this coarse ordering of the students, and again these can give
rise to inefficiencies by preventing mutually beneficial trades.
Applying the MCC mechanism instead of DA eliminates such
inefficiencies.

In the GCPS and MCC mechanisms (as described here -- they are actually
much more flexible) each student has been assigned a \emph{safe
school} to which she is guaranteed admission if no other school
accepts her.  It must be feasible to assign each student to her safe
school, as would typically be the case if each student's safe school
was the one whose district contains her residence.  Each student
submits a ranking of the schools she likes at least as much as the
safe school, with the safe school at the bottom.

\norev

The mechanism operates over the unit
interval $[0,1]$ of time.  At each moment each student is increasing
(at unit speed) her probability of receiving a seat in the favorite
school that is still available to her.  At time $1$ each student has a
probability distribution over the schools.

A school becomes unavailable if the total probability of it that has
been assigned is equal to its capacity.  A more subtle possibility is
that it has only enough remaining capacity to meet the needs of the
students for which it is the safe school, and who can no longer
consume any school that they prefer to it.  This can also happen with
sets of schools.  For example, if the schools in some region have only
enough remaining capacity to serve the students who can only receive
additional probability of schools in that region, then these schools
become unavailable to other students.  The precise description of
these constraints, and the proof that they are the only constraints,
are a big part of the mathematical magic underlying the GCPS
mechanism, and for an exact understanding you will need to read the
other paper.

Suppose that at a certain time the schools in some region have only
enough remaining capacity to serve the students who cannot receive
additional probability of schools outside that region.  The
continuation of the allocation process splits into two parts, one for
the schools in that region and the students who can now only be
assigned probability of one of them, and another for the remaining
schools and students.  The two subprocesses have the same form as the
original problem, which allows us to have an algorithm that is
recursive, in the sense that it calls itself on subproblems.  Sadly,
the days when recursive algorithms were thought to be supercool are
now in the distant past, but the idea can still be quite useful.

The GCPS mechanism has some small scope for manipulation by
misreporting of preferences.  Suppose that you are very near to
indifferent between two schools.  It can happen that depending on
which one you say you prefer, the total amount of time receiving
probability of one or the other will change. Typically the difference
between the two amounts of time will be quite small because you are
competing with many other students for probability in these schools.
Nevertheless, if you really really really don't care very much about
receiving more probability of the school you like less, manipulation
can be beneficial.  Clearly this isn't very important in itself, and
(in contrast with the Boston mechanism) you don't need to worry about
whether other students are manipulating in this way.  Overwhelmingly,
the basic structure of the GCPS mechanism makes it \emph{obvious} that
a student will be given what she asks for, to the extent possible, so
her best strategy is to ask for what she wants.

\subsection{Organization of the Remainder}

\norev

a
new algorithm for school choice, along with its theoretical
foundations.  This algorithm has been implemented (using the C
programming language) in the software package \emph{GCPS Schools} as
an executable \texttt{gcps}, which passes from a school choice problem
(as described below) to a matrix specifying, for each student-school
pair, the probability that the student is assigned to the school.  The
software package also contains two other executables \texttt{purify}
and \texttt{make\_ex}.  The first of these passes from a matrix of
assignment probabilities to a random pure assignment whose probability
distribution averages to the given matrix of probabilities.  The
second program generates example school choice problems of the sort
that might occur in large school districts.  These programs provide
the basic computational resources required to apply our mechanism, and
perhaps in some cases they will suffice.  However, the primary hope is
that the underlying code will be a useful starting point for further
software development.

\norev

This document describes these programs, from the point of view of a
user.  It doesn't assume that the reader has already read our paper,
but of course we are leaving out lots of relevant information.
Instructions for downloading and setting up the software, and doing a
test run, are given in Appendix \ref{app:DownloadInstall}.  It would
probably be a good idea to follow those instructions now, or before
reading too far into this guide, but the main body of this guide does
not assume that you have done so.  The body also does not assume that
the reader knows the C programming language, but some language
features become relevant in Appendix \ref{app:Code}.

\section{For the User}

In the remainder of the main body of this document we look at the
software from the point of view of school administrator (or perhaps an
administrator's tech support person) who wants to know how to use the
software to come up with an assignment of students to schools.  We'll
assume that the software has already been downloaded and installed, by
following the directions in Appendix \ref{app:DownloadInstall}, and
that you have now opened a command line terminal and are wondering
what to do.

\subsection{Input Files}

To begin with we describe a simple example of an input file.

\begin{obeylines}\texttt{
/* This is a sample introductory comment. */
There are 4 students and 3 schools
The vector of quotas is (1,2,1)
The priority matrix is
     1     1     1
     1     0     1
     1     1     1
     1     1     1
The students numbers of ranked schools are (3,2,3,3)
The preferences of the students are
1:  1  2  3  
2:  1  3  
3:  1  2  3  
4:  1  2  3  
  }
\end{obeylines}

\medskip

\emph{GCPS Schools} input files begin with a comment between
\texttt{/*} and \texttt{*/}.  This is purely for your convenience.
The comment can be of any length, and provide whatever information is
useful to you, but it is mandatory insofar as the computer will insist
that the first two characters of the file are \texttt{/*}, and it will
only start extracting information after it sees the \texttt{*/}.

The computer divides the remainder of the file into ``generalized
white space'' and ``tokens.''  Generalized white space includes the
usual white space characters (spaces, tabs, and new lines), and in
addition `\texttt{(}', `\texttt{)}', and `\texttt{,}' are treated as
white space.  Tokens are contiguous sequences of characters without
any of the generalized white space characters.  Tokens are either
prescribed words, nonnegative integers, positive integers, or student
or school tags (a student or school number followed by `\texttt{:}').
Everything must be more or less exactly as shown above, modulo white
space, so, for example, the first line must not be \texttt{There are 3
  students and 1 school}, but it could be \texttt{There are 3 students
  and \ \ 1 schools}.  If one of the GCPS executables tries to read a
file and finds a violation of the format requirements it will print a
short statement describing the problem and quit.

The first line after the comment gives the quotas (i.e., the
capacities) of the schools, so school 2 has two seats, and the other
two schools each have one seat.  Here we see the convenience of making
`\texttt{(}', `\texttt{)}', and `\texttt{,}' white space characters:
otherwise we would have had to write \texttt{The vector of quotas is 1
  2 1}.

Our treatment of priorities is somewhat different from what is typical
in the school choice literature, where the priority is thought of as
the ``utility'' the school gets from a student, and is often required
to come from a strict ranking of the students.  At this stage a
student's priority at a school is 1 if she is allowed to attend the
school, and may be assigned a seat there, and otherwise it is 0.  Such
priorities are said to be \emph{dichotomous}.  (We'll talk about more
complicated priorities later.)  A student's priority at a school may
be 0 because she is not qualified (it is a single sex school for boys,
or her test scores are too low) or it may be 0 because the student
prefers a seat at her safe school and can insist on receiving a seat
at a school that she likes at least as much.

The next line provides information (for each student, the number of
schools for which she has priority 1) that the computer could figure
out for itself, but we prefer to confirm that whatever person or
software prepared the input knew what they were doing.  After that
come the students' preferences: for each student, that student's tag
followed by the schools she might attend, listed from best to worst.
The collection of information provided by such an input file is a
\emph{school choice problem}.  We recommend file names for input files
that end with \texttt{.scp}, but the software does not enforce this.

\subsection{\texttt{gcps\_di}}

We now imagine that you've done the hard work of figuring out what
each school's capacity is, which schools each student is eligible to
attend, assigning a safe school to each student, and getting each
student to provide the rank ordered list of schools she is eligible
for and likes at least as much as her safe school.  All this
information has been encoded in an input file \texttt{my.scp} that is
in the current directory, and the executable \texttt{gcps\_di} is also
in this directory.  In a Unix environment (if you use another
operating system you will need to make the necessary adjustments) the
next step could be to issue the command:
\begin{obeylines}
  \texttt{
    \%\% ./gcps\_di my.scp > my.mat
    }
\end{obeylines}
\bigskip

In the Unix OS the user has a \texttt{PATH}, which is a list of
directories.  When you issue a command from the command line, the
first item on the command line is the name of the command, and the
computer goes through the directories in the \texttt{PATH} looking for
an executable with that name.  For security reasons some flavors of
Unix do not put the current directory (denoted by \texttt{\ .\ }) in
the \texttt{PATH}, so you need to tell the computer that that is where
you want it to look.  Thus \texttt{./gcps\_di my.scp} is telling the
computer to apply the version of \texttt{gcps\_di} in the current
directory to the file \texttt{my.scp}.  (It is also possible to run
\texttt{gcps\_di} without specifying a file, in which case it will
look for a file \texttt{schools.scp} in the current directory.)  By
default the output of the command goes to the screen, which is not
very useful, so \texttt{> my.mat} \emph{redirects} the output to a
file \texttt{my.mat}, which is created in the current directory by
this command.  We recommend that files produced by \texttt{gcps\_di}
have filenames ending in \texttt{.mat} (for \emph{matrix}), but the
software does not enforce this.

If \texttt{my.scp} is the input file above, \texttt{./gcps\_di my.scp}
gives the output shown below.
\medskip
\begin{obeylines}\texttt{
/* This is a sample introductory comment. */
The allocation is:
\ \ \ \ \ 1:    \    2:  \      3:
1:      0.25     0.67     0.08
2:      0.25     0.00     0.75
3:      0.25     0.67     0.08
4:      0.25     0.67     0.08
}
\end{obeylines} \noindent

\smallskip \noindent
Note that the sum of the entries in each row is 1 and the sum of the
entries in each school's column is that school's quota.  In general
the sum of the schools' quotas may exceed the number of students, in
which case we only require that the sum of the entries in each
school's column does not exceed that school's quota. An assignment of
probabilities with these properties --- each student has positive
probability only in schools they are eligible for, each student's
total assignment is 1, and no school is overassigned --- is a
\emph{feasible allocation}.

Our mechanism is descended from the \emph{probabilistic serial}
mechanism of \cite{bm01} for probabilistic allocation of objects,
which was generalized by \cite{bckm13aer} to the \emph{generalized
probabilistic serial} mechanism, and further generalized to the
\emph{generalized constrained probabilistic serial} mechanism by
\cite{balbuzanov22jet}.  These mechanisms are usually described in
terms of simultaneous eating: each school is thought of as a cake
whose size is its capacity, and at each moment during the unit
interval of time each student ``eats'' probability of the favorite
cake that is still available to her.  In our example each student
consumes probability of a seat in her favorite school (school 1) until
that resource is exhausted at time 0.25, at which point each student
switches to the next best thing.  This continues until school 2 is
also exhausted, after which all finish up by consuming probability of
a seat in school 3.

In general, at each time each student is consuming probability of a
seat at the favorite school among those that are still available to
her.  This continues until the first time that there is a set of
schools $P$ such that the remaining capacity is just sufficient to
meet the needs of the students in the set $J_P$ of students who no
longer have access to any schools outside of $P$.  At this point the
problem divides into two subproblems, one corresponding to the sets
$P$ and $J_P$ and the other corresponding to the complements of these
sets.  These problems have the same form as the original problem, and
can be treated algorithmically in the same way, so the algorithm can
descend recursively to smaller and smaller subproblems until a
feasible allocation has been fully computed.


\subsection{\texttt{purify}} \label{sec:Implementation}

Having generated the file \texttt{my.mat}, which is a matrix of
assignment probabilities, the next problem is to generate a random
assignment of students to schools that realizes these probabilities.
That is, we want to generate a random deterministic feasible
assignment of students to schools such that for each student
\texttt{i} and school \texttt{j}, the probability that \texttt{i}
receives a seat in \texttt{j} is the corresponding entry in
\texttt{my.mat}.  Doing this is called
\emph{implementation} by \cite{bckm13aer}. It can be accomplished by issuing the command:
\begin{obeylines}
  \texttt{
    \%\% ./purify my.mat > my.pur
    }
\end{obeylines}
\bigskip \noindent (It is also possible to run \texttt{purify} without
specifying an input file, in which case it will look for a file with
the name \texttt{allocate.mat}.)  As was the case with inputs to
\texttt{gcps\_di}, the requirements for input files for
\texttt{purify} are quite stringent, and are satisfied by the output
of \texttt{gcps\_di}.

The algorithm implemented by \texttt{purify} is a special case of an
algorithm of \cite{bckm13aer}.  We can illustrate it by applying it to
the allocation we obtained above:

\begin{obeylines}\texttt{
\ \ \ \ \ $A$:    \     $B$:  \      $C$:
1:      0.25     0.67     0.08
2:      0.25     0.00     0.75
3:      0.25     0.67     0.08
4:      0.25     0.67     0.08
}
\end{obeylines} \noindent
(Here and below we leave out we leave out the introductory comment and
the ``The allocation is':'' which are superfluous, and we change the
school names from numbers to letters for the sake of clarity.)

We consider a cyclic path alternating between students and
schools, say $1 \to C \to 3 \to A \to 1$, such that the entries of the
matrix for $(1,C)$, $(3,C)$, $(3,A)$, and $(1,A)$ are all strictly
between $0$ and $1$.  If we add $0.08$ to the entries for $(1,C)$ and
$(3,A)$ while subtracting $0.08$ from the entries for $(3,C)$ and
$(1,A)$, we obtain
\begin{obeylines}\texttt{
\ \ \ \ \ $A$:    \     $B$:  \  $C$:
1:      0.17     0.67     0.17
2:      0.25     0.00     0.75
3:      0.33     0.67     0.00
4:      0.25     0.67     0.08
}
\end{obeylines} \noindent
(Recall that $0.08$, $0.17$, and $0.33$ are really $\tfrac{1}{12}$,
$\tfrac16$ and $\tfrac13$.)  This is also a feasible allocation.  We
could also subtract $0.08$ from the entries for $(1,C)$ and $(3,A)$
while adding $0.08$ the entries for $(3,C)$ and $(1,A)$, thereby
obtaining the feasible allocation
\begin{obeylines}\texttt{
\ \ \ \ \ $A$:    \     $B$:  \      $C$:
1:      0.33     0.67     0.00
2:      0.25     0.00     0.75
3:      0.17     0.67     0.17
4:      0.25     0.67     0.08
}
\end{obeylines} \noindent

The computer chooses between these two matrices by flipping a coin.
If heads, it then generates a random pure allocation that averages to
the first matrix, and if tails, then it produces a random pure
allocation that averages to the second matrix.  The average of the
overall distribution of pure allocations is the matrix we started
with.

Note that each of these matrices has one more zero than the original
matrix.  The general idea is to continue in this manner, at each stage
passing randomly from a given matrix to one of two matrices that have
the given matrix as a convex combination, and that each have at least
one more entry that is either zero or one.

\subsection{\texttt{make\_ex}} \label{sec:MakeEx}

Development of this sort of software requires testing under at least
somewhat realistic conditions.  The utility \texttt{make\_ex} produces
examples of input files for \texttt{gcps} that reflect the
geographical dispersion of schools within school districts with many
schools, and the idiosyncratic nature of school quality and student
preferences.

One of the files produced by \texttt{make\_ex} begins as follows:
\begin{obeylines}\texttt{
    /* This file was generated by make\_ex with 20 schools,
    4 students per school, capacity 5 for all schools,
    school valence standard deviation 1.00,
    and idiosyncratic standard deviation 1.00. */
}
\end{obeylines} \noindent
In this example there are 20 schools that are spaced evenly around a
circle of circumference 20.  Since there are 4 students per school,
there are 80 students.  Their homes are also spaced evenly around the
circle.  Each student's safe school is the school closest to her home.
A student's utility for a school is the sum of the school's valence
and an idiosyncratic shock, minus the distance from the student's home
to the school.  Each school's valence is a normally distributed random
variable with mean 0.0 and standard deviation 1.0, and for each
student-school pair the idiosyncratic shock is a normally distributed
random variable with mean 0.0 and standard deviation 1.0.  All of
these random variables are independent.  The program passes from the
utilities to an input for \texttt{gcps} by finding the ranking, for
each student, of the schools for which the student's utility is at
least as large as the utility of the safe school.

Near the beginning of the file \texttt{example.c} there are the following lines:
\begin{obeylines}\texttt{
  int no\_schools = 20;
  int no\_students\_per\_school = 4;
  int school\_capacity = 5;
  double school\_valence\_std\_dev = 1.0;
  double idiosyncratic\_std\_dev = 1.0;
}
\end{obeylines} \noindent
Even for someone who knows nothing about the C programming language,
this is pretty easy to understand.  The keywords \texttt{int} and
\texttt{double} are data types for integers and floating point
numbers.  Thus \texttt{no\_schools},
\texttt{no\_students\_per\_school}, and \texttt{school\_capacity} are
integers, while \texttt{school\_valence\_std\_dev} and
\texttt{idiosyncratic\_std\_dev} are floating point numbers.  Each
line assigns a value to some variable.  If you would like to generate
examples with different parameters, the way to do that is to first
change the parameters by editing \texttt{example.c}, then run
\texttt{make} to compile \texttt{make\_ex} with the new parameters,
and finally issue a command like \texttt{make\_ex > my\_file.scp}
which runs \texttt{make\_ex} and redirects the output to the file
\texttt{my\_file.scp}.  For example, to diminish the relative
importance of travel costs one can increase
\texttt{school\_valence\_std\_dev} and
\texttt{idiosyncratic\_std\_dev}.

This illustrates an important point concerning the relationship
between this software and its users.  Most softwares you are familiar
with have interfaces with the user that neither require nor allow the
user to edit the source code, but to create such an interface here
would be counterproductive. It would add complexity to the source code
that had nothing to do with the underlying algorithms.  More
importantly, the main purpose of this software is to provide a
starting point for the user's own programming effort in adapting it to
the particular requirements and idiosyncratic features of the user's
school choice setting.  Our algorithms are not very complicated, and
someone familiar with C should hopefully not have a great deal of
difficulty figuring out what is going on and then bending it to her
purposes.  Starting to look at and edit the source code as soon as
possible is a first step down that road.


\section{Finer Priorities} \label{sec:Priorities}

To appreciate the issue discussed in this section one needs to
understand some of the history of other school choice mechanisms.
Instead of matching students to seats in schools, it is perhaps more
intuitive to consider matching a finite set of boys with a finite set
of girls, who each have strict preferences over potential partners and
remaining single.

The boy-proposes version of the famous deferred acceptance algorithm
begins with each boy proposing to his favorite girl, if there is one
he prefers to being alone.  Each girl rejects all proposals that are
less attractive than being alone, and if she has received more than
one acceptable proposal, she holds on to her favorite and rejects all
the others.  In each subsequent round, each boy who was rejected in
the previous round proposes to his favorite among the girls who have
not yet rejected him, if one of these is acceptable. Each girl now has
a number of new proposals, and possibly the proposal she brought
forward from the previous round.  She retains her favorite of these,
if it is acceptable, rejecting all others.  This procedure is repeated
until there is a round with no rejections, at which point each girl
holding a proposal pairs up with the boy whose proposal she is
holding. This mechanism was first proposed in the academic literature
by \cite{GaSh62}, but it turned out that it had already been used for
several years to match new graduates of medical schools with
residencies.  For almost twenty years it has been used in school
matching, with the students proposing and the seats in the various
schools rejecting, and it is now in widespread use around the world.

The key point for us is that this mechanism is not well defined unless
both sides have strict preferences.  In the context of school
matching, the schools' preferences are called \emph{priorities}.  If
these priorities are not actual reflections of society's values, this
can result in inefficiency.  For example, if Carol School's priorities
rank Bob above Ted while Alice School's priorities rank Ted above Bob,
then we could have an assignment in which Bob envies Ted's seat at
Alice School while Ted envies Bob's seat at Carol School.  This sort
of inefficiency can be quantitatively important, and a major advantage
of our mechanism is that it is efficient, in an even stronger sense
than not allowing outcomes in which improving trades are possible.

However, there are cases in which the schools' priorities do reflect
actual values.  In China, for example, each student's priority at all
schools is the score on a standardized test.  A consequence of this,
under deferred acceptance, is that, in effect, each school has an exam
score cutoff, accepting all students above the cutoff, rejecting all
students below the cutoff, and randomizing (roughly speaking) over
students right at the cutoff.  Our main concern in this section is to
explain how our mechanism can achieve similar outcomes.

The first point is that our input files can have a richer structure
than our original example suggests, as illustrated by the input below.
The priorities can be arbitrary nonnegative integers.  A student
having a priority of 0 at a particular school is understood as
indicating that the student cannot be assigned there, either because
she is not qualified or because she prefers her safe school.  A
student's safe school can be indicated by giving the student the
highest possible priority at that school.  The computer passes from
this input to a school choice problem in which the priority of a
student at a school is 1 if her priority in the input is not less than
the school's priority threshold, and it is 0 otherwise, each school's
priority threshold is set to 1, and each student's preference is
truncated by eliminating schools she is not eligible for.  Applying
this procedure to the input below gives our original example.

\begin{obeylines}\texttt{
/* This is a sample introductory comment. */
There are 4 students and 3 schools
The vector of quotas is (1,2,1)
The priority matrix is
     5     6     9
     2     2     9
     5     4     9
     3     4     9
The students numbers of ranked schools are (3,3,3,3)
The preferences of the students are
1:  1  2  3  
2:  1  2  3  
3:  1  2  3  
4:  1  2  3  
The priority thresholds of the schools are
1   3   5   
  }
\end{obeylines}

It is possible to repeatedly adjust the schools' priority thresholds
to achieve a desired effect.  For example, suppose there are two
selective schools, and the school district would like it to be the
case that a well qualified student is almost certain to receive a seat
in one of them if that is what she wants, and at the same time these
schools do not have more than a small amount of unused capacity.  One
may raise the priority threshold of one of these schools if many
students are receiving some probability of admission and lower the
threshold if its seats are not being filled.  Of course changing the
priority threshold at one of the schools will effect demand for the
other school, so repeated adjustment of the priority thresholds of all
the schools may be required to achieve a desirable result.
(Automating this iterative adjustment process may require the
development of a version of \texttt{gcps} that can accept parameter
inputs, without editing the source code.  This should be a simple task
for an experienced C programmer.)

%%%------------------------------------------------------------------------------------
%%%------------------------------------------------------------------------------------
\bibliographystyle{agsm}
\bibliography{pa_ref}


\begin{appendix}

\section{Downloading and Setting Up} \label{app:DownloadInstall}

Here we give step-by-step instructions for downloading the code,
compiling the executables, and starting to use them.  I am going to
assume a Unix command line environment, which could be a terminal in
Linux, the terminal application in MacOS, or some third flavor of
Unix.  (There are probably easy enough ways to do these things in
Windows (I wouldn't know) but a Windows user can also just get
Cygwin.)

First, in a browser, open the url
\begin{obeylines}
  \texttt{
    https://github.com/Coup3z-pixel/SchoolOfChoice/
    }
\end{obeylines}

\bigskip \noindent You will see a list of directories and files.
Clicking on the filename \texttt{gcps\_schools.tar} will take you to a
page for that file.  On the line beginning with \texttt{Code} you will
see a button marked \texttt{Raw}.  Clicking on that button will
download the file to your browser.  Move it to a suitable directory.

We use the \texttt{tar} command to
extract its contents, then go into the directory \texttt{GCPS} that this action creates:
\begin{obeylines}
  \texttt{
    \%\% tar xvf gcps\_schools.tar
    \%\% cd gcps\_schools
    }
\end{obeylines}
\bigskip

To compile the executables we need the tools \texttt{make} and
\texttt{gcc}, and we can check for their presence using
the command \texttt{which}:
\begin{obeylines}
  \texttt{
    \%\% which make
    /usr/bin/make
    \%\% which gcc
    /usr/bin/gcc
    }
\end{obeylines}
\bigskip \noindent
If you don't have them, you will need to get them.

Assuming all is well, we issue the command \texttt{make} and see the text that the command directs to the screen:
\begin{obeylines}
  \texttt{
    \%\% make
    gcc -I. -Wall -Wextra  -c normal.c
    gcc -o make\_ex example.c normal.o -lm
    gcc -I. -Wall -Wextra  -c parser.c
    gcc -I. -Wall -Wextra  -c subset.c
    gcc -I. -Wall -Wextra  -c cee.c
    gcc -I. -Wall -Wextra  -c schchprob.c
    gcc -I. -Wall -Wextra  -c partalloc.c
    gcc -I. -Wall -Wextra  -c push\_relabel.c
    gcc -I. -Wall -Wextra  -c gcps\_solver.c
    gcc -o gcps solve.c normal.o parser.o subset.o cee.o
    \ \ schchprob.o partalloc.o push\_relabel.o gcps\_solver.o -lm
    gcc -I. -Wall -Wextra  -c implement.c
    gcc -o purify purify.c normal.o parser.o subset.o partalloc.o
    \ \ implement.o -lm
    } 
\end{obeylines}
\bigskip
\noindent
Each line of output above corresponds to one of the commands in the
\texttt{makefile}, and each of the commands in the makefile specifies
an object to be constructed (either an object (that is, a \texttt{.o}
file) or an executable) the resources that are required to construct
it, and the command that constructs it.  (There is also an object
\texttt{all}, which requires the three executables.  When make is
asked to construct something (e.g., the command \texttt{make gcps}) it
first makes all the resources that that thing requires, so
\texttt{make all} will result in constructing all of the executables
at once. Because \texttt{make}'s default behavior is to construct the
first object in the \texttt{makefile}, \texttt{make} has the same
effect as \texttt{make all}.  There is also an object \texttt{clean}.
The command \texttt{make clean} removes the objects and executables,
and also any of the \texttt{*\~} files that the \texttt{emacs} editor
leaves behind after a preexisting file has been edited. The command
\texttt{make clean} takes us back to the situation before
\texttt{make} was invoked, and after that you can issue the command
\texttt{make} again to see it all happen again.

There are now the executables \texttt{make\_ex}, \texttt{gcps}, and
\texttt{purify}.  On many Unix's these can invoked simply by typing
the executable name on the command line, but it may be the case that,
for security reasons, the current directory is not in the
\texttt{path} (the list of directories that the command line looks in
when a command is invoked) in which case you will need to type
\texttt{./make\_ex}, \texttt{./gcps}, and \texttt{./purify}.  We begin
with \texttt{make\_ex}:
\begin{obeylines}
  \texttt{
    \%\% make\_ex
    /* This file was generated by make\_ex with 2 schools, 3 students
    per school, capacity 4 for all schools, school valence standard
    deviation 1.00, and idiosyncratic standard deviation 1.00. */
There are 6 students and 2 schools
The vector of quotas is (4,4)
The priority matrix is
\ \ \     1    1
\ \ \     1    1
\ \ \     1    0
\ \ \     1    0
\ \ \     1    0
\ \ \     0    1
The students numbers of ranked schools are
(2,2,1,1,1,1)
The preferences of the students are
1:    1   2
2:    1   2
3:    1
4:    1
5:    1
6:    2
The priority thresholds of the schools are
1   1   
    }
\end{obeylines}
\bigskip
\noindent
\textbf{Warning:} If what you get looks a bit different, it may be
because your installation of C and mine have different random number
generators.

Let's redirect the output to the file \texttt{schools.scp}, then invoke \texttt{gcps}:
\begin{obeylines}
  \texttt{
    \%\% make\_ex > schools.scp
    \%\% gcps
  /* This is a sample introductory comment. */
There are 6 students and 2 schools
\ \ \ \ \ \ \ \ \           1:   \ \ \ \ \ \ \        2:
1:   0.50000000  0.50000000
2:   0.50000000  0.50000000
3:   1.00000000  0.00000000
4:   1.00000000  0.00000000
5:   1.00000000  0.00000000
6:   0.00000000  1.00000000
  }
\end{obeylines}
\bigskip



Finally, let's redirect the output of \texttt{gcps} to the file \texttt{allocate.mat}, then invoke \texttt{purify}:
\begin{obeylines}
  \texttt{
    \%\% gcps > allocate.mat
    \%\% purify
    /* This is a sample introductory comment. */
\ \ \ \          1:   2:
   1:    0 \ \    1
   2:    1 \ \    0
   3:    1 \ \    0
   4:    1 \ \    0
   5:    1 \ \    0
   6:    0 \ \    1
  }
\end{obeylines}
\bigskip

That's all there is to it! We've now been through a complete cycle,
and the rest is up to you.  If you feel like it, you may want to
experiment with different parameters for \texttt{make\_ex} by editing
the file \texttt{example.c}, as described in Section \ref{sec:MakeEx},
then running \texttt{make} again and going through the
\texttt{make\_ex}-\texttt{gcps}- \texttt{purify} cycle.  This will
give you an initial feel for how fast \texttt{gcps} is.  (It's
\textit{very} fast.)  But after reading the rest of this guide, you
may well have your own ideas concerning what to do next.

\section{About the Code} \label{app:Code}

As we have mentioned earlier, we hope that our code provides a useful
starting point for others, either contributing to the repository at
Github, or for applications to districts with particular features.
For this reason we have kept things as simple as possible, even if
that entails somewhat less convenience for the user.  In particular,
the input and output formats are inflexible, and some users will
probably want to develop more sophisticated interfaces.

In this Appendix we provide an overview of the code, passing from the
simpler and more basic files to increasingly higher levels, in each
case describing those features that might not be so obvious.  Our hope
is to ease the process of learning about the code by providing a level
of explanation in which the objects in the code are described in human
terms, and in relation to the earlier descriptions of the algorithms.
While reading the descriptions of the files below, the reader should
also be looking at the files themselves, and especially the header
(\texttt{*.h}) files.

Before diving into details, here are some general remarks.  First,
although we have used C rather than C++ (for a project as small as
this, the various advantages of C++ seem not worth the additional
complexity of that language) the code is object oriented in spirit,
being organized as interactions of objects given by \texttt{struct}s.
Most of the time objects are ``passed by reference'' to functions,
which means that instead of passing the object itself, what is passed
is a pointer to the object.  Understanding the pointer concept of C is
a prerequisite to any detailed understanding of the code.

With perhaps one or two exceptions, each object has a destroyer, which
frees the memory that stores the object's data, and for many objects
there is a way of printing the object.  These printing functions
provide the format of the output of \texttt{make\_ex}, \texttt{gcps},
and \texttt{purify}, and for other objects the printing functions can
be useful for debugging.  In all cases the code for these functions is
simple and straightforward, and printing and destroyer functions will
not be mentioned below.  When studying the code, the reader can mostly
ignore the many calls to destroyers, trusting that the allocation and
freeing of memory is being handled correctly.

In the C programming language, an $n$ element array is indexed by the
integers $0, \ldots, n-1$.  We always think of it as indexed by the
integers $1, \ldots, n$, so the $j^{\text{th}}$ component of
\texttt{vec} is \texttt{vec[j-1]}.  Similarly, the $(i,j)$ component
of a matrix \texttt{mat} is \texttt{mat[i-1][j-1]}.  While this is
perhaps not one of the most appealing features of C, and it certainly
adds bulk to the code, once you get used to it, in a curious way it
seems to enhance the readability of the code.

\subsection{\texttt{normal.h} and \texttt{normal.c}}

The function \texttt{min} computes the minimum of two doubles.  The
function \texttt{is\_integer} returns 1 (true) if the given double is
within one one millionth of an integer and 0 (false) otherwise.  In
general, throughout the code, two floating point numbers are regarded
as equal if they differ by less that one millionth.  This prevents
rounding error from creating a spurious impression that two numbers
differ.  Incidently, the reason that the numbers in the output of
\texttt{gcps} have many digits is that an output of \texttt{gcps} must
be an accurate input for \texttt{purify}, so \texttt{gcps} shouldn't
(for example) print 0.99 instead of 0.99999999. The functions
\texttt{uniform} and \texttt{normal} provided uniformly distributed
(in $[0,1]$) and normally distributed (for mean 0 and standard
deviation 1) pseudorandom numbers.

\subsection{\texttt{example.c}}

The file \texttt{example.c} contains the \texttt{main} function of
\texttt{make\_ex}, which contains all of the code that is involved in
generating an example.  Although the code is somewhat lengthy, the
process is a straight line:
\begin{enumerate}
  \item[(a)] Locate the schools and students around the circle.
  \item[(b)] Compute the matrix of distances between students and schools.
  \item[(c)] Generate normally distributed random valences for the schools.
  \item[(d)] The utility of student \texttt{i} for school \texttt{j}
    is the valence of \texttt{j} plus a normally distributed
    \texttt{(i,j)}-idiosyncratic shock minus the distance from
    \texttt{i} to \texttt{j}.
  \item[(e)] Each student's safe school is (roughly) the one that is closest.
  \item[(f)] Student \texttt{i}'s priority at school \texttt{j} is one
    if its utility for $i$ is not less than the utility of
    \texttt{i}'s safe school, and otherwise it is zero.
  \item[(g)] The preference of student \texttt{i} is the list of
    schools of priority one, in order of decreasing utility.
\end{enumerate}

\subsection{\texttt{parser.h} and \texttt{parser.c}}

Two parsing functions \texttt{sch\_ch\_prob\_from\_file} and
\texttt{allocation\_from\_file} are declared in \texttt{parser.h}.  As
their names suggest, these functions read data from files,
constructing, respectively, a school choice problem
(\texttt{sch\_ch\_prob}) and an allocation (\texttt{partial\_alloc}).
A valid input file has an opening comment, which begins with
\texttt{/*} and ends with \texttt{*/}, and a body.  In the body, in
addition to the usual white space characters (space, tab, and newline)
the characters `\texttt{(}', `\texttt{)}', and `\texttt{,}' are
treated as white space.  The body is divided into whitespace and
tokens, which are sequences of adjacent characters without any white
space that are preceeded and followed by white space.

Everything in \texttt{parser.c} is easy to understand.  There are
numerous functions checking that the verbal tokens are the ones that
are expected, and quitting with an error message if one of them
isn't. This makes the code extremely verbose and thoroughly
amateurish.  If the reader kindly refrains from looking in
\texttt{parser.c}, this author will be spared considerable
embarrassment.

\subsection{\texttt{subset.h} and \texttt{subset.c}}

One may represent a subset of $\{1, \ldots, n\}$ as an $n$-tuple of
0's and 1's, or as a list of its elements.  The first of these is
given by \texttt{subset}, which, in addition to the $n$-tuple
\texttt{indicator} of elements of $\{0,1\}$, keeps track of the number
of elements of the subset and the number of elements of the set it is
a subset of.  The second representation is given by \texttt{index}, in
which \texttt{no\_elements} is the number of elements of the subset
(not the containing set) and \texttt{indices} is a strictly increasing
\texttt{no\_elements}-tuple of elements of $\{1, \ldots,
\text{\texttt{large\_set\_size}}\}$.  The \texttt{index}
representation can be much more efficient when we are dealing with
little subsets of big sets.

The function
\texttt{index\_of\_subset} passes from the first to the second, and
\texttt{subset\_of\_index} goes in the other direction.  (Since an
\texttt{index} does not know the size of the set it is a subset of,
that piece of data is a required argument.) There is no index
representation of the empty set, and if \texttt{subset\_of\_index}
receives the empty set as an argument, it will complain and halt the
program.

A \texttt{index\_list} is a linked list of subsets in \texttt{index}
form.  

Mostly the functions in \texttt{subset.h} have self explanatory
titles, with code that is not hard to understand.  There may now be
some functions that are not used elsewhere, as I have not made an
effort to eliminate such functions when they may prove useful later,
and are illustrative of what is possible.

\subsection{\texttt{cee.h} and \texttt{cee.c}}

The most general notion of a \emph{communal endowment economy} (CEE)
has a finite set of \emph{agents}, a finite set of \emph{objects}, a
vector of (nonegative) \emph{requirements} for the agents, a vector of
(nonnegative) \emph{quotas} for the objects, and a matrix that
specifies, for each agent and each object, the maximum amount of the
object that the agent may consume.  A \emph{partial allocation} is
matrix that specifies a nonnegative consumption of each object by each
agent.  A partial allocation is \emph{feasible} if: a) no agent
consumes more than the allowed quantity of any object; b) the sum of
each agents allocation is her requirement; c) the sum of each object's
allocations does not exceed the object's quota.  In the context of
school choice the agents are \emph{students}, the objects are
\emph{schools}, each student's requirement is one, and each school has
\emph{quota}, which is the number of seats it can fill.

In the software, a CEE is part of the algorithm's input, and CEE's
also occur as the allocation unfolds, so two CEE structs are defined
in \texttt{cee.h}, \texttt{input\_cee} and \texttt{process\_cee}.  An
\texttt{input\_cee} has a matrix \texttt{priority} that specifies a
nonnegative integer for each student-school pair.  At the most basic
level the \texttt{priority} for a student-school pair would be $1$ if
the student was eligible to attend the school and weakly preferred it
to her safe school.  But this matrix presents an opportunity to encode
the priority of each student at each school, which we can use to
reduce to the more basic level, as we explain below.

In \texttt{process\_cee} the quotas and \texttt{maximums} for each
student-school pair are floating point numbers, and there is an
additional number \texttt{time\_remaining} that encodes the amount of
time that remains for allocation for a \texttt{process\_cee}.  The
\texttt{maximums} will often either agree with \texttt{time\_remaining} or be
zero, but the algorithm allows them to be in between.

With \texttt{maximums} that are strictly between $0$ and
\texttt{time\_remaining} the notion of a \emph{critical pair} becomes
a bit more complex.  A pair $(J,P)$ consisting of a set of students
$J$ and a set of schools $P$ is \emph{critical} if the only way to
meet the requirement of the agents in $J$ is to give each student in
$J$ her maximum of each of the schools outside $P$, and to give all of
the seats in schools in $P$ only to students in $J$.  When the pair
$(J,P)$ becomes critical for the CEE of as yet unallocated resources,
the computation recursively descends to two subprocesses, one for the
agents in $J$ and all the resources that they might possibly consume
(which is the minimum needed to meet their requirements) and the other
for students in the complement of $J$ and schools in the complement of
$P$.  The functions \texttt{critical\_sub\_process\_cee} and
\texttt{crit\_compl\_sub\_process\_cee} compute the CEE's of these
subprocesses.

\subsection{\texttt{schchprob.h} and \texttt{schchprob.c}}

A \emph{school choice problem} combines a CEE, which may be thought of
as describing the outcomes that are physically possible, with
preferences for the students and priority thresholds for the schools.
A student is \emph{eligible} for a school if her priority at that
school is at or above the school's priority threshold. A student's
(strict) preference is the list of the schools she is eligible for,
going from best to worst.  For convenience we keep track of each
student's number of eligible schools.

There are two types of school choice problem, \texttt{input\_scp} and
\texttt{process\_scp}, which differ according to whether the
underlying CEE is an \texttt{inpu\_cee} or a \texttt{process\_cee},
and whether the schools have \texttt{priority\_threshold}'s.

The function \texttt{process\_scp\_from\_input} passes from an
\texttt{input\_sch\_ch\_prob} to a derived \texttt{sch\_ch\_prob}.  It
converts the quotas from integers to floating point numbers, and its
sets \texttt{time\_remaining} to 1.0.  For each student and school,
the student's \texttt{maximums} at the school is $1.0$ if the input
\texttt{priority} is not less than the school's
\texttt{priority\_threshold}, and positive (that is, the
\texttt{priority} and the \texttt{priority\_threshold} are not both
zero) and otherwise it is $0.0$.  For each student the
\texttt{no\_eligible\_schools} of the output \texttt{process\_scp} is
the \texttt{no\_eligible\_schools} of the input \texttt{input\_scp}
less the number of schools with positive input \texttt{priority} that
was below the school's threshold, and the output \texttt{preferences}
are the input \texttt{preferences} restricted to the remaining
schools.

Using \texttt{critical\_sub\_process\_cee} and
\texttt{crit\_compl\_sub\_process\_cee}, the functions
\texttt{critical\_sub\_process\_scp} and
\texttt{crit\_compl\_sub\_process\_scp} compute the derived
\texttt{process\_scp}'s when a pair $(J,P)$ becomes critical.  In the
\texttt{process\_scp} given by \texttt{crit\_compl\_sub\_process\_scp}
the quotas of schools in the complement of $P$ are reduced by the
\texttt{maximums} for these schools of the students in $J$.

\subsection{\texttt{partalloc.h} and \texttt{partalloc.c}}

In a \texttt{partial\_alloc} for \texttt{no\_students} students and
\texttt{no\_schools} schools, \texttt{allocations} is a matrix that
specifies an amount \texttt{allocations[i-1][j-1]} of school
\texttt{j} to student \texttt{i}, i.e., a probability that $i$
receives a set in $j$, for each \texttt{i} and \texttt{j}.  A
\texttt{pure\_alloc} has the same structure, but now
\texttt{allocations[i-1][j-1]} is an integer that should be zero or
one, and for each student \texttt{i} there should be exactly one
school \texttt{j} such that \texttt{allocations[i-1][j-1]} is one.

A \texttt{partial\_alloc} is \emph{feasible} if the total amount
assigned to each student is $1$ and the total assigned amount of each
school is not more than the school's quota.  A \texttt{partial\_alloc}
is \emph{possible} if it is (component-wise) less than or equal to
some feasible allocation.  The algorithm we are implementing
allocates, at unit speed, increments of each student's favorite school
to her until it encounters some face of the polytope of possible
allocations, at which point some pair $(J,P)$ is critical and there is
recursive descent to subproblems, as described above.

The detection of faces of the polytope of possible allocations is
aided by keeping track of a feasible allocation, called the
\texttt{feasible\_guide}, that is (component-wise) greater than or
equal to the allocation we are computing.  The allocation we are
computing, and \texttt{feasible\_guide}, are piecewise linear
functions of time, and the algorithm moves from one endpoint of a
linear piece to another by computing a direction \texttt{theta} of
motion for \texttt{feasible\_guide}, then finding the maximum amount
of time \texttt{delta} that the allocation can be increased by adding
favorites to the allocation while moving \texttt{feasible\_guide}
according to \texttt{theta} before some constraint becomes binding.
The functions \texttt{augment\_partial\_alloc} and
\texttt{adjust\_feasible\_guide} adjust the computed allocation and
\texttt{feasible\_guide} accordingly.

When a pair $(J,P)$ becomes critical the \texttt{feasible\_guide}'s of
the derived subproblems are obtained from \texttt{feasible\_guide} by
restriction, using the functions \texttt{left\_feasible\_guide} and
\texttt{right\_feasible\_guide}.  After the allocations of the
subproblems have been computed, the function
\texttt{increment\_partial\_alloc} combines them with the allocation
for the larger problem that has already been computed.

\subsection{\texttt{push\_relabel.h} and \texttt{push\_relabel.c}}
\label{subsec:PushRelabel}

The push-relabel algorithm of \cite{GoTa88} is implemented in
\texttt{push\_relabel.h} and \texttt{push\_relabel.c}.  The code
straightforwardly follows the description of the algorithm in Appendix
\ref{sec:Algorithms}, and the best way to approach it is to read that
description, then examine the code.

\subsection{\texttt{gcps\_solver.h} and \texttt{gcps\_solver.c}}
\label{subsec:Solver}

The files \texttt{gcps\_solver.h} and \texttt{gcps\_solver.c}
implement the algorithm for computing the GCPS allocation.  This
algorithm is also described in Appendix \ref{sec:Algorithms}, and
again the best way to approach it is examine the code only after the
description has been read and understood.  In this case it is best to
work backwards from the ends of \texttt{gcps\_solver.h} and
\texttt{gcps\_solver.c}, because that follows a top-down understanding
of how the algorithm is implemented.

\subsection{\texttt{implement.h} and \texttt{implement.c}}

The code of the algorithm going from a fractional allocation to a
random pure allocation whose distribution has the given allocation as
its average follows the description in Section
\ref{sec:Implementation}.  The \texttt{nonintegral\_graph} derived
from the given allocation is an undirected graph with an edge between
a student and a school if the student's allocation of the school is
strictly between zero and one, and an edge between a school and the
sink if the total allocation of the school is not an integer.  The
function \texttt{graph\_from\_alloc} has the given allocation as its
input, and its output is the derived \texttt{nonintegral\_graph}.

Especially for large school choice problems, we expect the
\texttt{nonintegral\_graph} to be quite sparse, so it can be
represented more compactly, and be easier to work with, if we encode
it by listing the neighbors of each node.  The \texttt{stu\_sch\_nbrs}
member of \texttt{neighbor\_lists} is a list of \texttt{no\_students}
lists, where the \texttt{stu\_sch\_nbrs[i-1]} are arrays of varying
dimension. We set \texttt{stu\_sch\_nbrs[i-1][0] = 0} in order to have
a place holder that allows us to not have an array with no entries
when \texttt{i} has no neighbors.  The actual neighbors of \texttt{i}
are
$$\text{\texttt{stu\_sch\_nbrs[i-1][1],...,stu\_sch\_nbrs[i-1][stu\_no\_nbrs[i-1]]}}.$$
The members \texttt{sch\_no\_nbrs} and \texttt{sink\_sch\_nbrs} follow
this pattern, except that in the latter case there is just a single
list.  The member \texttt{sch\_sink\_nbrs} is a
\texttt{no\_schools}-dimensional array of integers with
\texttt{sch\_sink\_nbrs[j-1] = 1} if there is an edge connecting
\texttt{j} and the \texttt{sink} and \texttt{sch\_sink\_nbrs[j-1] = 0}
otherwise.  To pass from a \texttt{nonintegral\_graph} to its
representation as a \texttt{neighbor\_lists} we apply
\texttt{neighbor\_lists\_from\_graph}.

A cycle in the \texttt{nonintegral\_graph} is a linked list of
\texttt{path\_node}'s.  The function \texttt{find\_cyclic\_path}
implements the algorithm for finding a cycle that we described in
Section \ref{sec:Implementation}.  Given a cycle,
\texttt{bound\_of\_cycle} computes the smallest ``alternating
perturbation,'' in one direction or the other, of the entries of (the
pointee of) \texttt{my\_alloc} that turns some component of the
allocation, or some total allocation of a school, into an integer.
For such an \texttt{adjustment} the function
\texttt{cyclic\_adjustment} updates the allocation, and it calls the
functions \texttt{student\_edge\_removal} and
\texttt{sink\_edge\_removal} to update \texttt{neighbor\_lists}.
When \texttt{graph\_is\_nonempty(my\_lists) = 0} (false) the entries
of \texttt{my\_alloc} are doubles that are all very close to integers,
and the function \texttt{pure\_allocation\_from\_partial} passes to
the associated \texttt{pure\_alloc}.  The function
\texttt{random\_pure\_allocation} is the master function that
supervises the whole process.

\subsection{\texttt{solve.c} and  \texttt{purify.c}}

The files \texttt{solve.c} and \texttt{purify.c} contain the
\texttt{main} functions of the executables \texttt{gcps} and
\texttt{purify} respectively.  These \texttt{main} functions are
mostly simple and straightforward.

In \texttt{solve.c} there are \texttt{int*} variables
\texttt{segments}, \texttt{splits}, \texttt{pivots}, and
\texttt{h\_sum}.  These are, respectively, the number of linear
segments of the piecewise linear function $(p,\barp)$ described in
Appendix \ref{sec:Algorithms}, the number of time the algorithm
descends recursively to two derived subproblems, the number of times
that the algorithm modifies $\theta$, as described iin Appendix
\ref{sec:Algorithms}, and the sum of the indices $h$ that arise in
paths $i_0,j_1, i_1, \ldots, i_h, j_h$ that are used to pivot.  These
provide interesting information about the algorithm's performance, and
there is a sample print statement below for them.

\section{The Principal Algorithms} \label{sec:Algorithms}

In this appendix we provide brief descriptions of the two principal
algorithms.  These are intended mainly to help the reader figure out
what is going on in the code.  For more information the reader is
referred to the papers that are the sources of these algorithms.

\subsection{Push-Relabel}

This subsection describes the push-relabel algorithm of \cite{GoTa88}.
We first describe the general setting of networks and flows, then we
describe the algorithm, and in the next subsection we describe the
specialized setting in which it is applied in our software.

Let $(N,A)$ be a directed graph ($N$ is a finite set of \emph{nodes}
and $A \subset N \times N$ is a set of \emph{arcs}) with distinct
distinguished nodes $s$ and $t$, called the \emph{source} and
\emph{sink} respectively.  We assume that $(n,s), (t, n) \notin A$ for
all $n \in N$.

A \emph{preflow} is a function $f \colon N \times N \to \Re$ such that:
\begin{enumerate}
  \item[(a)] for all $n$ and $n'$,  if $(n,n') \notin A$, then $f(n,n') \le 0$.
  \item[(b)] for all $n$ and $n'$,  $f(n,n') = - f(n',n)$ (antisymmetry); 
  \item[(c)] $\sum_{n' \in N} f(n',n) \ge 0$ for all $n \in N \setminus \{s,t\}$. 
\end{enumerate}
If neither $(n,n')$ nor $(n',n)$ is in $A$, then (a) and (b) imply
that $f(n,n') = 0$.  Note that $f(s,n), f(n,t) \ge 0$ for all $n \in
N$.  In conjunction with the other requirements, (c) can be understood
as saying that for each $n$ other than $s$ and $t$, the total flow
into $n$ is greater than or equal to the total flow out.

A preflow $f$ is a \emph{flow} if $\sum_{n' \in N} f(n,n') = 0$ for
all $n \in N \setminus \{s,t\}$.  In this case antisymmetry and this
condition imply that
$$0 = \sum_{n' \in N}\sum_{n \in N} f(n,n') = \sum_{n \in N} f(n,s) + \sum_{n' \in N} f(n,t),$$
so we may define \emph{value} of $f$ to be
$$|f| = \sum_{n \in N} f(s,n) = \sum_{n \in N} f(n,t).$$

A \emph{capacity} is a function $c \colon N \times N \to [0,\infty]$
such that $c(n,n') = 0$ whenever $(n,n') \notin A$.  A \emph{cut} is a
set $S \subset N$ such that $s \in S$ and $t \in S^c$ where $S^c = N
\setminus S$ is the complement.  For a capacity $c$, the
\emph{capacity} of $S$ is
$$c(S) = \sum_{(n,n') \in S \times S^c} c(n,n').$$

A preflow $f$ is \emph{bounded} by a capacity $c$ if $f(n,n') \le
c(n,n')$ for all $(n,n')$.  It is intuitive, and not hard to prove
formally, that if $f$ is a flow bounded by $c$ abd $S$ is a cut for
$c$, then $|f| \le c(S)$, so the maximum value of any flow is not
greater than the minimum capacity of a cut.  The max-flow min-cut
theorem \citep{FoFu56} asserts that these two quantities are equal.

The computational problems of finding the maximum flow or a minimal
cut for a network $(N,A)$ and a capacity $c$ are very well studied,
and many algorithms have been developed.  The push-relabel algorithm
is relatively simple, and certainly fast enough for our purposes.
(The literature continues to advance, and algorithms (e.g.,
\cite{CKLGS22}) with better asymptotic worst case bounds have been
developed.)
  
Let $f \colon N \times N \to \Re$ be a preflow that is bounded by $c$.
The \emph{excess} of $f$ at $n$ is $$e_f(n) = \sum_{n' \in N} f(n',n).$$
Of course $e_f(n) \ge 0$, and $f$ is a flow if and only if $e_f(n) =
0$ for all $n \in N \setminus \{s,t\}$.  The \emph{residual capacity}
of $(n,n')$ is $$r_f(n,n') = c(n,n') - f(n,n').$$  We say that $(n,n')$
is a \emph{residual edge} if $r_f(n,n') > 0$.  This can happen either
because $c(n,n') > f(n,n') \ge 0$ or because $f(n,n') < 0$.

A \emph{valid labelling} for $f$ and $c$ is a function $d \colon N \to
\{0,1,2,\ldots\} \cup \{\infty\}$ such that $d(t) = 0$ and $d(n) \le
d(n') + 1$ whenever $(n,n')$ is a residual edge.  We say that $n \in N
\setminus \{s,t\}$ is \emph{active} for $f$ and $d$ if $d(n) < \infty$
and $e_f(n) > 0$.

The algorithm is initialized by setting $d(s) = |N|$, $d(n) = 0$ for
all other $n$, $f(s,n) = c(s,n)$ for all $n$ such that $(s,n) \in A$,
and setting $f(n,n') = 0$ for all other $n$ and $n'$.  The algorithm
then consists of repeatedly applying the following two
\emph{elementary operations}, in any order, until there is no longer
any valid application of them:
\begin{enumerate}
  \item[(a)] $\mathrm{Push}(n,n')$ is valid if $n$ is active, $(n,n')
    \in A_f$ and $d(n') = d(n) - 1$.  The operation resets $f(n,n')$
    to $f(n,n') + \delta$ and $f(n',n)$ to $f(n',n) - \delta$ where
    $\delta = \min\{e_f(n),r_f(n,n')\}$.
  \item[(b)] $\mathrm{Relabel}(n)$ is valid if $n$ is active and $d(n)
    \le d(n')$ for all $n'$ such that $(n,n') \in A_f$.  The operation
    resets $d(n)$ to $\infty$ if there is no $n'$ such that $(n,n')
    \in A_f$ (this never actually happens) and otherwise it resets
    $d(n)$ to $1 + \min_{n' : (n,n') \in A_f} d(n')$.
\end{enumerate}
One intuitive understanding of the algorithm is that we imagine excess
as water flowing downhill, so that $d(n)$ can be thought of as a
height, (Goldberg and Tarjan offer a somewhat different intuition in
which $d$ is a measure of distance.) We think of $\mathrm{Push}(n,n')$
as moving $\delta$ units of excess from a node $n$ to an adjacent node
$n'$ that is one step lower.  The operation $\mathrm{Relabel}(n)$ is
valid when there is excess ``trapped'' at $n$, and this operation
increases $d(n)$ to the largest value allowed by the definition of a
valid labelling, which is the smallest value such that there is a
neighboring node the excess can flow to.

Based on the description above, it is not obvious that the
push-relabel algorithm is, in fact, an algorithm in the sense of
always terminating, nor is it obvious that it can only terminate at a
maximum flow.  Goldberg and Tarjan's proofs of these facts are subtle
and interesting, and their paper is recommended to the curious reader.

\subsection{School Choice Communal Endowment Economies}

A \emph{school choice communal endowment economy} (CEE) is a quadruple
$E = (I,O,q,g)$ in which $I$ is a nonempty finite set of
\emph{students}, $O$ is a nonempty finite set of \emph{schools}, $q
\in \Re_+^O$, and $g \in \Re_+^{I \times O}$.  For $i \in I$ and $j
\in O$ we say that $q_j$ is the \emph{quota} of $j$, and that $g_{ij}$
is \emph{$i$'s $j$-max}.

We apply the push-relabel algorithm to a particular directed graph
$(N_E,A_E)$ in which the set of nodes is $$N_E = \{s\} \cup I \cup O
\cup \{t\}.$$ For $i \in I$ and $j \in O$ let $a_i = (s,i)$, $a_{ij} =
(i,j)$, and $a_j = (j,t)$, and let
$$A_E = \{\, a_i : i \in I \,\} \cup \{\, a_{ij} : i \in I, j \in O
\,\} \cup \{\, a_j : j \in O \,\}.$$ Let $c_E$ be the capacity in
which $c_E(a_i) = 1$ for all $i$, $c_E(a_{ij}) = g_{ij}$ for all $i$
and $j$, and $c_E(a_j) = q_j$ for all $j$.  It turns out that when the
push-relabel algorithm is applied to a graph of this form, it is
possible to speed it up by initializing $d$ by setting $d(s) = 2|O| +
2$ and $d(n) = 0$ for all other $n$.  Roughly (this is not the place
to explain the details) this works because $2|O| + 2$ is an upper
bound on the number of nodes on a simple (no repeating nodes) path
from $s$ to $t$ when $|O| \le |I|$.

An \emph{allocation} for $E$ is a matrix $p \in \Re_+^{I \times O}$.
A \emph{partial allocation} for $E$ is an allocation $p$ such that
$\sum_j p_{ij} \le 1$ for all $i$, $\sum_i p_{ij} \le q_j$ for all
$j$, and $p_{ij} \le g_{ij}$ for all $i$ and $j$. A \emph{feasible
allocation} is a partial allocation $m$ such that $\sum_j m_{ij} =
1$ for all $i$.  A \emph{possible allocation} is an allocation $p$
such that there is a feasible allocation $m$ such that $p \le m$.  

If $p$ is an allocation, there is a unique flow $f_p$ such that
$f_p(a_{ij}) = p_{ij}$ for all $i$ and $j$ that has $f_p(a_i) = \sum_j
p_{ij}$ for all $i$ and $f_p(a_j) = \sum_i p_{ij}$ for all
$j$. Evidently $p$ is a feasible allocation if and only if $f_p$ is
bounded by $c_E$ and $f_p(a_i) = 1$ for all $i$, which is the case if
and only if $|f_p| = |I|$.  Conversely, if $f$ is a flow bounded by
$c_E$ with $|f| = |I|$ and thus $f(a_i) = 1$ for all $i$, then setting
$p_{ij} = f(a_{ij})$ gives a feasible allocation $p$.  Thus there is a
feasible allocation if and only if the maximum value of a flow bounded
by $c_E$ is $|I|$.  Although our primary use of the push-relabel
algorithm is to compute a feasible allocation, it also provides an
efficient method of determining whether a feasible allocation exists.

\subsection{Computing the GCPS Allocation}

We now describe how the GCPS allocation is computed.  We work with
a fixed school choice CEE $E = (I,O,q,g)$ and a profile $\succ \; =
(\succ_i)_{i \in I}$ of strict preferences over $O$. Let
$Q$ be the set of feasible allocations, and let $R$ be the set of
possible allocations. The allocation procedure is a piecewise linear function $p
\colon [0,1] \to R$ with $p(0) = 0$, $p(t) \in R \setminus Q$ for all
$t < 1$, and $p(1) \in Q$.  The \emph{GCPS allocation}
is $$GCPS(E,\succ) = p(1).$$

At each moment the trajectory of $p$ increases, at unit speed, each
student's assignment of her favorite school, among those that are
still available to her, while leaving other allocations fixed.  This
direction is adjusted when an student $i$'s assignment of a school $j$
reaches $g_{ij}$, and when $p$ arrives at one of the facets of $R$.
Suppose that $t^*$ is the first time such that $p(t^*)$ is in a facet
of $R$, so that there is a minimal critical pair $(J,P)$ for the
residual CEE, which we denote by $E - p(t^*)$. The GCPS allocation has
a recursive definition: for $t \in [t^*,1]$, $p(t)$ is, by definition,
the sum of $p(t^*)$ and the results of applying the allocation
procedure to the derived CEE's obtained by restricting $E - p(t^*)$ to
$J$ and $P$ and to the complements of $J$ and $P$, as described
earlier.

The main computational challenge is to detect when $p$ arrives at one
of the facets of $R$.  During the computation our algorithm computes
an auxilary piecewise linear function $\barp \colon [0,1] \to Q$ such
that $p(t) \le \barp(t)$ for all $t$.  We use the push-relabel
algorithm to compute $\barp(0)$.

The combined function $(p,\barp)$ is piecewise linear, and $[0,1]$ is
a finite union of intervals $[t_0,t_1], [t_1, t_2], \ldots, [t_{K-1},
  t_K]$, where $t_0 = 0$ and $t_K = 1$, such that on each interval
$[t_k,t_{k+1}]$ the derivative of $(p,\barp)$ is constant.  Suppose
that we have already computed $p(t_k)$ and $\barp(t_k)$.  For each
student $i$ we compute the set $\alpha_i(t_k)$ of schools that are still
possible for $i$, and we determine her $\succ_i$-favorite element
$e_i^k$.  Let $\theta^k \in \In^{I \times O}$ be the matrix such that
$\theta^k_{ij} = 1$ if $j = e_i^k$, and otherwise $\theta^k_{ij} = 0$.

There are now two possibilities.  The first is that for some $t' >
t_k$, $p(t_k) + \theta^k(t - t_k) \in R$ for all $t \in [t_k,t']$.  In
this case we will find a $\theta \in \In^{I \times O}$ such that for
some $t' > t_k$, $$\barp(t_k) + \theta(t -
t_k) \in Q \quad \text{and} \quad p(t_k) + \theta^k(t - t_k) \le \barp(t_k) + \theta(t
- t_k). \eqno{(*)}$$  and all $t \in [t_k,t']$.  Now $t_{k+1}$ is the first time after $t_k$ such
that one or more of the following holds:
\begin{enumerate}
  \item[a)] $p_{ie_i^k}(t_{k+1}) = g_{ie_i^k}$ for some $i$;
  \item[b)] $\barp(t_k) + \theta(t - t_k) \notin Q$ for $t > t_{k+1}$;
  \item[c)] for $t > t_{k+1}$ it is not the case that $p(t_k) +
    \theta^k(t - t_k) \le \barp(t_k) + \theta(t - t_k)$.
\end{enumerate}
For $t \in [t_k,t_{k+1}]$ we have determined that $p(t) = p(t_k) +
\theta^k(t - t_k)$, and we set $\barp(t) = \barp(t_k) + \theta(t -
t_k)$.  Having determined $p(t_{k+1})$ and $\barp(t_{k+1})$, we can
repeat the process.

The second possibility is that it is not possible to continue $p$, as
described above, without leaving $R$, because there is a critical pair
$(J,P)$ for the residual economy at time $t_k$.  In this case we find
such a pair, then descend recursively to the computation of the GCPS
allocations of derived subeconomies.

We now describe an algorithm that determines which of these
possibilities holds, In the first case it finds a satisfactory
$\theta$, and in the second case it finds a critical pair $(J,P)$.
Suppose that there is a $\theta \in \In^{I \times O}$ such that for
some $t' > t_k$, ($*$) holds for all $t \in [t_k,t']$.  The two
conditions in ($*$) together imply that $p(t_k) + \theta^k(t - t_k)
\in R$ for all $t \in [t_k,t']$, so the first possibility above holds,
and we can use $\theta$ to define the continuation of $\barp$.  The
algorithm may be thought of as a search for such a $\theta$.

For a given $\theta$, a $t' > 0$ as above exists if and only if
$\theta$ satisfies the following conditions:
\begin{enumerate} 
  \item[(a)] For each $i$ and $j$:
    \begin{enumerate}
      \item[(i)] If $o \notin \alpha_i$, then $\theta_{ij} = 0$.
      \item[(ii)] If $\barp_{ij}(t_k) = p_{ij}(t_k)$, then
        $\theta_{ij} \ge 0$, and if, in addition, $o = e^k_i$, then
        $\theta_{ij} \ge 1$.
      \item[(iii)] If $\barp_{ij}(t_k) = g_{ij}$, then $\theta_{ij} \le 0$.
    \end{enumerate}
  \item[(b)] For each $i$, $\sum_j \theta_{ij} = 0$.
  \item[(c)] For each $j$, if $\sum_i \barp_{ij}(t_k) = q_j$, then $\sum_i \theta_{ij} \le 0$.
\end{enumerate}


Our search for a suitable $\theta$ begins by defining an initial
$\theta \in \In^{I \times O}$ as follows.  For each $i$, if
$\barp_{ie^k_i}(t_k) > p_{ie^k_i}(t_k)$, then we set $\theta_{ij} = 0$
for all $j$.  If $\barp_{ie^k_i} = p_{ie^k_i}(t_k)$, then we set
$\theta_{ie^k_i} = 1$, we set $\theta_{ij_i} = -1$ for some $j_i \ne
e^k_i$ such that $\barp_{ij_i}(t_k) > p_{ij_i}(t_k)$, and we set
$\theta_{ij} = 0$ for all other $j$.  Evidently $\theta$ satisfies (a)
and (b).

Let $$\tP = \{\, j : \text{$\sum_i \barp_{ij}(t_k) = q_j$ and
  $\sum_i \theta_{ij} > 0$} \,\}.$$ If $\sum_{j \in \tP} \sum_i
\theta_{ij} \le 0$, then (c) holds.  Suppose that this is not the
case.  We now describe a construction that may or may not be possible.
When it is possible, it passes from $\theta$ to a $\theta' \in \In^{I
  \times O}$ satisfying (a) and (b) such that if $\tP' = \{\, j :
\text{$\sum_i \barp_{ij}(t_k) = q_j$ and $\sum_i \theta'_{ij} >
  0$} \,\}$, then $$\sum_{j \in \tP'} \sum_i \theta'_{ij} = \sum_{j
  \in \tP} \sum_i \theta_{ij} - 1. \eqno{(**)}$$ Repeating this construction will
eventually produce an element of $\In^{I \times O}$ satisfying
(a)--(c) unless, at some point, the construction becomes impossible.

Choose $j_0 \in \tP$.  We wish to find an integer $h \ge 1$, distinct
$i_1, \ldots, i_h$, and $j_1, \ldots, j_h$ such that $j_0,j_1, \ldots,
j_h$ are distinct, such that if we define $\theta'$ by
setting $$\theta'_{i_gj_{g-1}} = \theta_{i_gj_{g-1}} - 1 \quad
\text{and} \quad \theta'_{i_gj_g} = \theta_{i_gj_g} + 1$$ for $g = 1,
\ldots, h$ and $\theta'_{ij} = \theta_{ij}$ for all other $(i,j)$,
then $\theta'$ satisfies (a), (b), and ($**$).  Evidently $\theta'$
satisfies (i) if $j_{g-1}, j_g \in \alpha_{i_g}$ for all $g = 1,
\ldots, h$, it satisfies (ii) if, for each $g = 1, \ldots, h$, if
$\barp_{i_gj_{g-1}}(t_k) = p_{i_gj_{g-1}}(t_k)$, then
$\theta_{i_gj_{g-1}} > 0$ and $\theta_{i_gj_{g-1}} > 1$ if $j_{g-1} =
e^\succ_{i_g}$, and it satisfies (iii) if, for each $g = 1, \ldots,
h$, either $g_{i_gj_g} < \barp_{i_gj_g}(t_k)$ or $\theta_{i_gj_g} <
0$. Clearly $\theta'$ satisfies (b) if $\theta$ satisfies (b).  We
have $\sum_i \theta'_{ij_0} = \sum_i \theta_{ij_0} - 1$, $\sum_i
\theta'_{ij_g} = \sum_i \theta_{ij_g}$ for $g = 1, \ldots, h-1$, and
$\sum_i \theta'_{ij_h} = \sum_i \theta_{ij_h} + 1$, so $\theta'$
satisfies ($**$) if $\sum_i \barp_{ij_h}(t_k) < q_{j_h}$ or $\sum_i
\theta_{ij_h} < 0$.

We now describe the search for $j_0,j_1, \ldots, j_h$ and $i_1,
\ldots, i_h$. For $i \in I$ let
$$P(i) = \{\, j \in \alpha_i : \text{$\theta_{ij} < 0$
  if $\barp_{ij}(t_k) = g_{ij}$} \,\}.$$
For $j \in O$ let
$$J(j) = \{\, i : \text{$j \in \alpha_i$ and if
  $\barp_{ij}(t_k) = p_{ij}(t_k)$, then $\theta_{ij} > 0$ and
  $\theta_{ij} > 1$ if $j = e^\succ_i$} \,\}.$$ We define sets $P_0,
J_1, P_1, J_2, \ldots$ inductively, beginning with $P_0 = \{j_0\}$.
If $P_{g-1}$ has already been computed, we set $J_g = \bigcup_{j \in
  P_{g-1}} J(j)$. If $J_g$ has already been computed,
we set $P_g = \bigcup_{i \in J_g} P(i)$.  We continue
this construction until we arrive at an $h$ such that either $P_h =
P_{h-1}$ or there is a $j_h \in P_h$ such that $\sum_i
\barp_{ij_h}(t_k) < q_{j_h}$ or $\sum_i \theta_{ij_h} < 0$.

If there is such a $j_h$, we can find an $i_h \in J_h$ such that $j_h
\in P(i_h)$, then find an $j_{h-1} \in P_{h-1}$ such
that $i_h \in J(j_{h-1})$, then find an $i_{h-1} \in
J_{h-1}$ such that $j_{h-1} \in P(i_{h-1})$, and so
forth.  Continuing in this fashion produces $j_0,j_1, \ldots, j_h$ and
$i_1, \ldots, i_h$ as above.  Note that $j_h$ cannot be an element of
$P_{h-1}$ because the process would have terminated sooner.  Similarly
$i_h$ is not an element of $J_{h-1}$, $j_{h-1} \notin P_{h-1}$, and so
forth.  Therefore $j_0,j_1, \ldots, j_h$ are distinct and $i_1,
\ldots, i_h$ are distinct.

Now suppose that the construction terminates with $P_h = P_{h-1}$.
Let $J = \bigcup_h J_h$ and $P = \bigcup_h P_h$.  We have $\sum_i
\barp_{ij}(t_k) = q_j$ for all $j \in P$.  If $j \in P$ and $i \notin
J$, then $i \notin J(j)$, so $\barp_{ij}(t_k) = p_{ij}(t_k)$.  If $i
\in J$ and $j \notin P$, then $j \notin P(i)$, so $\barp_{ij}(t_k) =
g_{ij}$ if $j \in \alpha_i \setminus P$, and $\barp_{ij}(t_k) = g_{ij}
= 0$ if $j \notin \alpha_i$. Thus $\barp(t_k) - p(t_k)$ is a feasible
allocation for $E - p(t_k)$ that gives all of the resources in $P$ to
students in $J$, and it gives $g_{ij} - p_{ij}(t_k)$ to $i \in J$
whenever $j \in O \setminus P$. Clearly any feasible allocation also
has these properties, so $(J,P)$ is a critical pair for $E - p(t_k)$.

Summarizing, the algorithm repeatedly extends $p$ and $\barp$ to
intervals such as $[t_k,t_{k+1}]$ until $p(t_k)$ satisfies the GMC
equality for a pair $(J,P)$, at which point it descends
recursively. If $p(t_k)$ does not satisfy such a GMC inequality, it
finds a $\theta$ satisfying (a)--(c) by beginning with a $\theta$ that
satisfies (a) and (b) and repeatedly adjusting it until it also
satisfies (c).


\end{appendix}

\end{document}

