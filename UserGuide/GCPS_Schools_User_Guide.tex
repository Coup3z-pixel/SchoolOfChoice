\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amstext}
\usepackage{amssymb}
\usepackage{amsthm}
\makeatletter
\usepackage{graphicx,epsf}
\usepackage{times,float}
\usepackage{enumerate}
\usepackage[round,comma]{natbib}
\usepackage[colorlinks=true,citecolor=blue]{hyperref}
\usepackage{bm}
\usepackage{multirow}
%\usepackage{blkarray}
\usepackage{rotating}

\setlength{\textwidth}{6.4in} \setlength{\textheight}{8.5in}
\setlength{\topmargin}{-.2in} \setlength{\oddsidemargin}{.1in}
\renewcommand{\baselinestretch}{1.3}

\theoremstyle{definition}
\newtheorem{thm}{Theorem}
\newtheorem*{thm*}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{lem}{Lemma}
\newtheorem*{lem*}{Lemma}
\newtheorem{claim}{Claim}
\newtheorem{rem}{Remark}
\newtheorem{ex}{Example}
\newtheorem{fact}{Fact}
\newtheorem*{fact*}{Fact}
\newtheorem{remark}{Remark}


\newcommand{\rR}{\mathrel{R}}
\newcommand{\rP}{\mathrel{P}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\norev}{\medskip \centerline{\textbf{No Revisions Below}} \medskip}
\renewcommand{\Re}{\mathbb{R}}
\newcommand{\In}{\mathbb{Z}}

\newcommand{\bare}{\overline{e}}
\newcommand{\barl}{\overline{l}}

\newcommand{\bq}{\mathbf{q}}

\newcommand{\cE}{\mathcal{E}}
\newcommand{\cH}{\mathcal{H}}
\newcommand{\cM}{\mathcal{M}}
\newcommand{\cP}{\mathcal{P}}
\newcommand{\cX}{\mathcal{X}}

\newcommand{\dr}{{\dot r}}
\newcommand{\dq}{{\dot q}}
\newcommand{\dg}{{\dot g}}
\newcommand{\ddp}{{\dot p}}

\newcommand{\hA}{{\hat A}}
\newcommand{\hO}{{\hat O}}

\newcommand{\halpha}{{\hat \alpha}}

\newcommand{\ta}{{\tilde a}}
\newcommand{\te}{{\tilde e}}
\newcommand{\tn}{{\tilde n}}

\newcommand{\tB}{{\tilde B}}

\newcommand{\bark}{{\overline k}}
\newcommand{\bart}{{\overline t}}

\newcommand{\varep}{\varepsilon}

\newcommand{\bone}{\mathbf{1}}

\begin{document}

\title{GCPS Schools: A User's Guide}

\author{Andrew McLennan\footnote{School of Economics, University of
    Queensland, {\tt a.mclennan@economics.uq.edu.au}}}

\date{\today}

\maketitle

\begin{abstract}
This document provides a brief introduction to the software package GCPS Schools.
\end{abstract}

% \pagebreak

\section{Introduction}

In the paper ``An Efficient, Computationally Tractable School Choice
Mechanism'' (joint with Shino Takayama and Yuki Tamura) we describe a
new algorithm for school choice, along with its theoretical
foundations.  This algorithm has been implemented (using the C
programming language) in the software package \emph{GCPS Schools} as
an executable \texttt{gcps}, which passes from a school choice problem
(as described below) to a matrix specifying, for each student-school
pair, the probability that the student is assigned to the school.  The
software package also contains two other executables \texttt{purify}
and \texttt{make\_ex}.  The first of these passes from a matrix of
assignment probabilities to a random pure assignment whose probability
distribution averages to the given matrix of probabilities.  The
second program generates example school choice problems of the sort
that might occur in large school districts.  These programs provide
the basic computational resources required to apply our mechanism, and
perhaps in some cases they will suffice.  However, the primary hope is
that the underlying code will be a useful starting point for further
software development.

This document describes these programs, from the point of view of a
user.  It doesn't assume that the reader has already read our paper,
but of course we are leaving out lots of relevant information.
Instructions for downloading and setting up the software, and doing a
test run, are given in Appendix \ref{app:DownloadInstall}.  It would
probably be a good idea to follow those instructions now, or before
reading too far into this guide, but the main body of this guide does
not assume that you have done so.  The body also does not assume that
the reader knows the C programming language, but some language
features become relevant in Appendix \ref{app:Code}.

\section{\texttt{gcps}}

To begin with we describe a simple example of an input file, which the
application \texttt{gcps} expects to find in a file called
\texttt{schools.scp} in the current directory.  (If there is no such
file \texttt{gcps} simply complains and quits.)

\begin{obeylines}\texttt{
/* This is a sample introductory comment. */
There are 4 students and 3 schools
The vector of quotas is (1,2,1)
The priority matrix is
     1     1     1
     1     0     1
     1     1     1
     1     1     1
The students numbers of ranked schools are (3,2,3,3)
The preferences of the students are
1:  1  2  3  
2:  1  3  
3:  1  2  3  
4:  1  2  3  
The priority thresholds of the schools are
1   1   1   
  }
\end{obeylines}

\medskip

\emph{GCPS Schools} input files begin with a comment between
\texttt{/*} and \texttt{*/}.  This is purely for your convenience.
The comment can be of any length, and provide whatever information is
useful to you, but it is mandatory insofar as the computer will insist
that the first two characters of the file are \texttt{/*}, and it will
only start extracting information after it sees the \texttt{*/}.  The
computer divides the remainder into ``generalized white space'' (in
addition to spaces, tabs, and new lines, `\texttt{(}', `\texttt{)}',
and `\texttt{,}' are treated as white space) and ``tokens,'' which are
sequences of characters without any of the generalized white space
characters.  Tokens are either prescribed words, numbers, or student
tags (a student number followed by `\texttt{:}') and everything must
be more or less exactly as shown above, modulo white space, so, for
example, the first line must not be \texttt{There are 3 students and 1
  school}, but it could be \texttt{There are 3 students and \ \ 1
  schools}.

The next line gives the quotas (i.e., the capacities) of the schools,
so school 2 has two seats, and the other two schools each have one
seat.  Here we see the convenience of making `\texttt{(}',
`\texttt{)}', and `\texttt{,}' white space characters: otherwise we
would have had to write \texttt{The vector of quotas is 1 2 1}.

Our treatment of priorities is somewhat different from what is typical
in the school choice literature, where the priority is thought of as
the ``utility'' the school gets from a student, and is often required
to come from a strict ranking of the students.  At this stage a
student's priority at a school is either 1 if she is allowed to attend
the school, and may be assigned a seat there, and otherwise it is 0.
(We'll talk about more complicated priorities later.)  A student's
priority at a school may be 0 because she is not qualified (it is a
single sex school for boys, or her test scores are too low) or it may
be 0 because the student prefers a seat at her ``safe school'' (as we
explain below) and can insist on receiving a seat at a school that she
likes at least as much.

The next line provides information (for each student, the number of
schools for which she has priority 1) that the computer could figure
out for itself, but we prefer to confirm that whatever person or
software prepared the input knew what they were doing.  After that
come the students' preferences: for each student, that student's tag
followed by the schools she might attend, listed from best to worst.
Finally, there are the schools' minimum priorities for admission,
which in this context are all 1: a student is good enough to admit to
a school if her priority for that school is 1 and not
otherwise. (Again, we'll talk about more complex situations later.)
The collection of information provided by an input file is a
\emph{school choice problem}.

What the software does (primarily) is compute a matrix of assignment
probabilities.  For our particular example \texttt{gcps} gives the
output shown below.  (Here and below we are leaving out the sample
comment.)  Note that the sum of the entries in each row is 1 and the
sum of the entries in each school's column is that school's quota.  In
general the sum of the quotas may exceed the number of students, in
which case we require that the sum of the entries in each school's
column does not exceed that school's quota. An assignment of
probabilities with these properties --- each student's total
assignment is 1 and no school is overassigned --- is a \emph{feasible
allocation}.
\medskip
\begin{obeylines}\texttt{
The allocation is:
\ \ \ \ \ 1:    \    2:  \      3:
1:      0.25     0.67     0.08
2:      0.25     0.00     0.75
3:      0.25     0.67     0.08
4:      0.25     0.67     0.08
}
\end{obeylines} \noindent

\smallskip
Our mechanism is based on the ``simultaneous eating'' algorithm of
\cite{bm01} for probabilistic allocation of objects, as generalized by
\cite{balbuzanov22jet}.  In our example each student consumes
probability of a seat in her favorite school (school 1) until that
resource is exhausted at time 0.25, at which point each student
switches to the next best thing.  This continues until school 2 is
also exhausted, after which all finish up by consuming probability of
a seat in school 3.

This makes good sense if the schools simply fill up one by one, as in
this example, but is that always what happens?  Actually, no.  To help
understand this we first introduce a new concept, the ``safe school.''
The idea is that each student has one school, say the closest school
or the school that a sibling attends, to which she is guaranteed
admission if she insists.  Each student submits a ranking of the
schools that she is eligible for and (weakly) prefers to her safe
school, and her priority is 1 at those schools and 0 at all others.

Now suppose that there are two schools, say 1 and 2, that are quite
popular.  Some students have school 1 as their safe school, but prefer
2, and some students have school 2 as their safe school, but prefer 1.
There are also some students who have other safe schools, but prefer
either 1 or 2, or both.  As the students consume probability at their
favorite schools, there can come a time at which schools 1 and 2
together only have enough remaining capacity to serve the students who
can insist on going to one of these two schools, even though school 1
still has excess capacity if it can ignore the students who have 1 as
their safe school but prefer 2 and the students who have 2 as their
safe school but prefer 1, and similarly for school 2.  When this
happens we say that the set of schools $P = \{1,2\}$ has become
\emph{critical}.

At this time further consumption of capacity at schools 1 and 2 is
restricted to those students who cannot be assigned to other schools,
so further consumption of these schools is denied to students who do
not have 1 or 2 as their safe school, and also to students who have 1
or 2 as their safe school but prefer some third school that is still
available.  For each of the latter students the least preferred of the
schools she is willing to attend that is still available becomes the
new safe school.

More generally, let $P$ be a set of schools, and let $J_P$ be the set
of students whose priorities for all schools outside of $P$ are 0.
For any $i \in J_P$, a feasible allocation must assign probability 1
to student $i$ receiving a seat in $P$, so a necessary condition for
the existence of a feasible allocation is that the total capacity of
the schools in $P$ is not less than the number of students in $J_P$.
We call this condition the \emph{GMC} (generalized marriage condition)
\emph{for $P$}.  In fact this condition is sufficient for the
existence of a feasible allocation: \emph{if, for each set of schools
$P$, the GMC inequality for $P$ holds, then a feasible allocation
exists.}  This is not an obvious or trivial result, and a somewhat
more general version of it is one of the main points of our paper.
This result extends to situations where the resources have already
been partially allocated: \emph{if, for each set of schools $P$, the
total remaining capacity of the schools in $P$ is not less than the
total remaining demand of students in $J_P$} (where this set is
defined in relation to the students' current safe schools) \emph{then
there is an allocation of the remaining resources that gives a
feasible allocation.}

We can now describe the algorithm in a bit more detail.  At each time
each student is consuming probability of a seat at the favorite school
among those that are still available to her.  This continues until the
first time that there is a set of schools $P$ such that the remaining
capacity is just sufficient to meet the needs of the students in the
set $J_P$ of students who no longer have access to any schools outside
of $P$.  At this point the problem divides into two subproblems, one
corresponding to the sets $P$ and $J_P$ and the other corresponding to
the complements of these sets.  These problems have the same form as
the original problem, and can be treated algorithmically in the same
way, so the algorithm can descend recursively to smaller and smaller
subproblems until a feasible allocation has been fully computed.


\section{\texttt{purify}} \label{sec:Implementation}

Leaving out the initial comment, the output of \texttt{gcps} is a
matrix of assignment probabilities, as shown again below.
(Now, to minimize confusion, the schools are $A$, $B$, and $C$.)  
Generating a random deterministic assignment with a probability
distribution that averages to this matrix is called
\emph{implementation} by \cite{bckm13aer}.  The executable
\texttt{purify} reads a feasible matrix $m$ of assignment
probabilities from a file \texttt{allocate.mat}, which must have the
form of the output of \texttt{gcps}, and it produces a random
deterministic allocation with a suitable distribution, using an
algorithm of \cite{bckm13aer}, as it applies to our somewhat simpler
framework.


\begin{obeylines}\texttt{
\ \ \ \ \ $A$:    \     $B$:  \      $C$:
1:      0.25     0.67     0.08
2:      0.25     0.00     0.75
3:      0.25     0.67     0.08
4:      0.25     0.67     0.08
}
\end{obeylines} \noindent

\medskip
We can illustrate the algorithm using the feasible allocation shown
above.  We consider a cyclic path alternating between students and
schools, say $1 \to C \to 3 \to A \to 1$, such that the entries of the
matrix for $(1,C)$, $(3,C)$, $(3,A)$, and $(1,A)$ are all strictly
between $0$ and $1$.  If we add $0.08$ to the entries for $(1,C)$ and
$(3,A)$ while subtracting $0.08$ from the entries for $(3,C)$ and
$(1,A)$, we obtain
\begin{obeylines}\texttt{
\ \ \ \ \ $A$:    \     $B$:  \  $C$:
1:      0.17     0.67     0.17
2:      0.25     0.00     0.75
3:      0.33     0.67     0.00
4:      0.25     0.67     0.08
}
\end{obeylines} \noindent
(Recall that $0.08$, $0.17$, and $0.33$ are really $\tfrac{1}{12}$,
$\tfrac16$ and $\tfrac13$.)  This is also a feasible allocation.  We
could also subtract $0.08$ from the entries for $(1,C)$ and $(3,A)$
while adding $0.08$ the entries for $(3,C)$ and $(1,A)$, thereby
obtaining the feasible allocation
\begin{obeylines}\texttt{
\ \ \ \ \ $A$:    \     $B$:  \      $C$:
1:      0.33     0.67     0.00
2:      0.25     0.00     0.75
3:      0.17     0.67     0.17
4:      0.25     0.67     0.08
}
\end{obeylines} \noindent
Note that each of these matrices has one more zero than the original matrix.

The computer chooses between these two matrices by flipping a coin.
If heads, it then generates a random pure allocation that averages to
the first matrix, and if tails, then it produces a random pure
allocation that averages to the second matrix.  The average of the
overall distribution of pure allocations is the matrix we started
with.

We now give a more formal explanation of the algorithm.  There is a
directed graph whose nodes are the students, the schools, and a
\emph{sink}.  The graph has an arc from each student to each school,
and an arc from each school to the sink.  A \emph{flow} is an
assignment of a positive number to each arc such that for each
student, the sum of the flows to all schools is $1$, and for each
school the sum of the flows from all students is equal to the flow
from that school to the sink.  A matrix of assignment probabilities
$m$ has an associated flow $f$ in which the flow from each student to
each school is the probability that the student receives a seat in the
school, and the flow from the school to the sink is the sum of the
school's assignment probabilities.

There is a subgraph consisting of all arcs whose flows are not
integers.  A key point is that for any node that is an endpoint of one
of the arcs in the subgraph, there is another arc in the subgraph
that also has that node as an endpoint.  For each student, this is
obvious because the sum of the student's assignment probabilities is
one.  If the sum of the flows into a school is an integer, and one of
these flows is not an integer, then there must be another flow into
the school that is not an integer.  If the sum of the flows into a
school is not an integer, then one of the flows into the school is not
an integer, and the flow from the school to the sink is not an
integer.  The sum of the flows into the sink is the sum of the flows
out of the students, which is the number of students, hence an
integer, so if the flow from one of the schools to the sink is not an
integer, there must be another such school.

Consequently the subgraph has a \emph{cycle}, which is a sequence of
distinct nodes $n_1, \ldots, n_k$ such that $k > 2$ and, for each $i =
1, \ldots, k$, $n_i$ and $n_{i+1}$ are the endpoints of an arc in the
subgraph.  (We are treating the indices as integers mod $k$, so $k + 1
= 1$.)  The algorithm for finding a cycle (whose correctness is the
proof of the existence of a cycle) works in an obvious manner.
Beginning with $n_1$ and $n_2$ that are the endpoints of an arc in the
subgraph, it finds $n_3 \ne n_1$ such that $n_2$ and $n_3$ are the
endpoints of an arc in the subgraph.  In general, after finding $n_i$
such that $n_{i-1}$ and $n_i$ are the endpoints of an arc in the
subgraph, the algorithm asks whether there is $j = 1, \ldots, i-2$
such that $n_i = n_j$, in which case $n_j, \ldots, n_{i-1}$ is the
desired cycle, and otherwise it finds $n_{i+1} \ne n_{i-1}$ such that
$n_i$ and $n_{i+1}$ are the endpoints of an arc in the subgraph.
Since there are finitely many nodes, the process must eventually halt.

Given a cycle $n_1, \ldots, n_k$, for each $i = 1, \ldots, k$ we say
that $n_in_{i+1}$ is a \emph{forward arc} if $n_i$ is a student and
$n_{i+1}$ is a school, or if $n_i$ is a school and $n_{i+1}$ is the
sink, and otherwise we say that $n_{i+1}n_i$ is a \emph{backward arc}.
For any real number $\delta$, if we modify $f$ by adding a constant
$\delta$ to the flow of each forward arc while subtracting $\delta$
from the flow of each backward arc, the result $f^\delta$ is a new
flow, because for each student the sum of outward flows is $1$, and
for each school the sum of flows from students to the school is the
flow from the school to the sink.

Let $\alpha$ be the smallest positive number such that $f^\alpha$ has
at least one more integer entry than $f$, and let $\beta$ be the
smallest positive number such that $f^{-\beta}$ has at least one more
integer entry than $f$.  Then $f = \tfrac{\beta}{\alpha +
  \beta}f^\alpha + \tfrac{\alpha}{\alpha + \beta}f^{-\beta}$.  Let
$m^\alpha$ and $m^{-\beta}$ be the restrictions of $f^\alpha$ and
$f^{-\beta}$ to the arcs from students to schools.  It is easy to see
that $m^\alpha$ and $m^{-\beta}$ are feasible allocations: their
entries lie in $[0,1]$, and the sums of the entries for each student
and each school are the corresponding sums for $m$.

The algorithm passes from $m$ and $f$ to $m^\alpha$ and $f^{\alpha}$
with probability $\tfrac{\beta}{\alpha + \beta}$, and to $m^{-\beta}$
and $f^{-\beta}$ with probability $\tfrac{\alpha}{\alpha + \beta}$.
Whichever of $m^\alpha$ and $m^{-\beta}$ is chosen, if it is not a
deterministic assignment, then the process is repeated.  We claim that
the the average of the resulting distribution of pure allocations is
$m$.  Since $m = \tfrac{\beta}{\alpha + \beta}m^\alpha +
\tfrac{\alpha}{\alpha + \beta}m^{-\beta}$, this is clear if $m^\alpha$
and $m^{-\beta}$ are pure allocations, and in general it follows from
induction on the number of nonintegral entries of $m$.

The code for the algorithm described above is in \texttt{implement.c},
which has the associated header file \texttt{implement.h}.  The file
\texttt{purify.c} contains a high level sequence of commands that
execute the algorithm.

\section{\texttt{make\_ex}} \label{sec:MakeEx}

Development of this sort of software requires testing under at least
somewhat realistic conditions.  The utility \texttt{make\_ex} produces
examples of input files for \texttt{gcps} that reflect the
geographical dispersion of schools within school districts with many
schools, and the idiosyncratic nature of school quality and student
preferences.

One of the files produced by \texttt{make\_ex} begins as follows:
\begin{obeylines}\texttt{
    /* This file was generated by make\_ex with 20 schools,
    4 students per school, capacity 5 for all schools,
    school valence standard deviation 1.00,
    and idiosyncratic standard deviation 1.00. */
}
\end{obeylines} \noindent
In this example there are 20 schools that are spaced evenly around a
circle of circumference 20.  Since there are 4 students per school,
there are 80 students.  Their homes are also spaced evenly around the
circle.  Each student's safe school is the school closest to her home.
A student's utility for a school is the sum of the school's valence
and an idiosyncratic shock, minus the distance from the student's home
to the school.  Each school's valence is a normally distributed random
variable with mean 0.0 and standard deviation 1.0, and for each
student-school pair the idiosyncratic shock is a normally distributed
random variable with mean 0.0 and standard deviation 1.0.  All of
these random variables are independent.  The program passes from the
utilities to an input for \texttt{gcps} by finding the ranking, for
each student, of the schools for which the student's utility is at
least as large as the utility of the safe school.

Near the beginning of the file \texttt{example.c} there are the following lines:
\begin{obeylines}\texttt{
  int no\_schools = 20;
  int no\_students\_per\_school = 4;
  int school\_capacity = 5;
  double school\_valence\_std\_dev = 1.0;
  double idiosyncratic\_std\_dev = 1.0;
}
\end{obeylines} \noindent
Even for someone who knows nothing about the C programming language,
this is pretty easy to understand.  The keywords \texttt{int} and
\texttt{double} are data types for integers and floating point
numbers.  Thus \texttt{no\_schools},
\texttt{no\_students\_per\_school}, and \texttt{school\_capacity} are
integers, while \texttt{school\_valence\_std\_dev} and
\texttt{idiosyncratic\_std\_dev} are floating point numbers.  Each
line assigns a value to some variable.  If you would like to generate
examples with different parameters, the way to do that is to first
change the parameters by editing \texttt{example.c}, then run
\texttt{make} to compile \texttt{make\_ex} with the new parameters,
and finally issue a command like \texttt{make\_ex > my\_file.scp}
which runs \texttt{make\_ex} and redirects the output to the file
\texttt{my\_file.scp}.  For example, to diminish the relative
importance of travel costs one can increase
\texttt{school\_valence\_std\_dev} and
\texttt{idiosyncratic\_std\_dev}.

This illustrates an important point concerning the relationship
between this software and its users.  Most softwares you are familiar
with have interfaces with the user that neither require nor allow the
user to edit the source code, but to create such an interface here
would be counterproductive. It would add complexity to the source code
that had nothing to do with the underlying algorithms.  More
importantly, the main purpose of this software is to provide a
starting point for the user's own programming effort in adapting it to
the particular requirements and idiosyncratic features of the user's
school choice setting.  Our algorithms are not very complicated, and
someone familiar with C should hopefully not have a great deal of
difficulty figuring out what is going on and then bending it to her
purposes.  Starting to look at and edit the source code as soon as
possible is a first step down that road.


\section{Finer Priorities} \label{sec:Priorities}

To appreciate the issue discussed in this section one needs to
understand some of the history of other school choice mechanisms.
Instead of matching students to seats in schools, it is perhaps more
intuitive to consider matching a finite set of boys with a finite set
of girls, who each have strict preferences over potential partners and
remaining single.

The boy-proposes version of the famous deferred acceptance algorithm
begins with each boy proposing to his favorite girl, if there is one
he prefers to being alone.  Each girl rejects all proposals that are
less attractive than being alone, and if she has received more than
one acceptable proposal, she holds on to her favorite and rejects all
the others.  In each subsequent round, each boy who was rejected in
the previous round proposes to his favorite among the girls who have
not yet rejected him, if one of these is acceptable. Each girl now has
a number of new proposals, and possibly the proposal she brought
forward from the previous round.  She retains her favorite of these,
if it is acceptable, rejecting all others.  This procedure is repeated
until there is a round with no rejections, at which point each girl
holding a proposal pairs up with the boy whose proposal she is
holding. This mechanism was first proposed in the academic literature
by \cite{GaSh62}, but it turned out that it had already been used for
several years to match new graduates of medical schools with
residencies.  For almost twenty years it has been used in school
matching, with the students proposing and the seats in the various
schools rejecting, and it is now in widespread use around the world.

The key point for us is that this mechanism is not well defined unless
both sides have strict preferences.  In the context of school
matching, the schools' preferences are called \emph{priorities}.  If
these priorities are not actual reflections of society's values, this
can result in inefficiency.  For example, if Carol School's priorities
rank Bob above Ted while Alice School's priorities rank Ted above Bob,
then we could have an assignment in which Bob envies Ted's seat at
Alice School while Ted envies Bob's seat at Carol School.  This sort
of inefficiency can be quantitatively important, and a major advantage
of our mechanism is that it is efficient, in an even stronger sense
than not allowing outcomes in which improving trades are possible.

However, there are cases in which the schools' priorities do reflect
actual values.  In China, for example, each student's priority at all
schools is the score on a standardized test.  A consequence of this,
under deferred acceptance, is that, in effect, each school has an exam
score cutoff, accepting all students above the cutoff, rejecting all
students below the cutoff, and randomizing (roughly speaking) over
students right at the cutoff.  Our main concern in this section is to
explain how our mechanism can achieve similar outcomes.

The first point is that our input files can have a richer structure
than our original example suggests, as illustrated by the input below.
The priorities can be arbitrary nonnegative integers.  A student
having a priority of 0 at a particular school is understood as
indicating that the student cannot be assigned there, either because
she is not qualified or because she prefers her safe school.  A
student's safe school can be indicated by giving the student the
highest possible priority at that school.  The computer passes from
this input to a school choice problem in which the priority of a
student at a school is 1 if her priority in the input is not less than
the school's priority threshold, and it is 0 otherwise, each school's
priority threshold is set to 1, and each student's preference is
truncated by eliminating schools she is not eligible for.  Applying
this procedure to the input below gives our original example.

\begin{obeylines}\texttt{
/* This is a sample introductory comment. */
There are 4 students and 3 schools
The vector of quotas is (1,2,1)
The priority matrix is
     5     6     9
     2     2     9
     5     4     9
     3     4     9
The students numbers of ranked schools are (3,3,3,3)
The preferences of the students are
1:  1  2  3  
2:  1  2  3  
3:  1  2  3  
4:  1  2  3  
The priority thresholds of the schools are
1   3   5   
  }
\end{obeylines}

It is possible to repeatedly adjust the schools' priority thresholds
to achieve a desired effect.  For example, suppose there are two
selective schools, and the school district would like it to be the
case that a well qualified student is almost certain to receive a seat
in one of them if that is what she wants, and at the same time these
schools do not have more than a small amount of unused capacity.  One
may raise the priority threshold of one of these schools if many
students are receiving some probability of admission and lower the
threshold if its seats are not being filled.  Of course changing the
priority threshold at one of the schools will effect demand for the
other school, so repeated adjustment of the priority thresholds of all
the schools may be required to achieve a desirable result.
(Automating this iterative adjustment process may require the
development of a version of \texttt{gcps} that can accept parameter
inputs, without editing the source code.  This should be a simple task
for an experienced C programmer.)

%%%------------------------------------------------------------------------------------
%%%------------------------------------------------------------------------------------
\bibliographystyle{agsm}
\bibliography{pa_ref}


\begin{appendix}

\section{Downloading and Setting Up} \label{app:DownloadInstall}

Here we give step-by-step instructions for downloading the code,
compiling the executables, and starting to use them.  I am going to
assume a Unix command line environment, which could be a terminal in
Linux, the terminal application in MacOS, or some third flavor of
Unix.  (There are probably easy enough ways to do these things in
Windows (I wouldn't know) but a Windows user can also just get
Cygwin.)

First, in a browser, open the url
\begin{obeylines}
  \texttt{
    https://github.com/Coup3z-pixel/SchoolOfChoice/
    }
\end{obeylines}

\bigskip \noindent You will see a list of directories and files.
Clicking on the filename \texttt{gcps\_schools.tar} will take you to a
page for that file.  On the line beginning with \texttt{Code} you will
see a button marked \texttt{Raw}.  Clicking on that button will
download the file to your browser.  Move it to a suitable directory.

We use the \texttt{tar} command to
extract its contents, then go into the directory \texttt{GCPS} that this action creates:
\begin{obeylines}
  \texttt{
    \%\% tar xvf gcps\_schools.tar
    \%\% cd gcps\_schools
    }
\end{obeylines}
\bigskip

To compile the executables we need the tools \texttt{make} and
\texttt{gcc}, and we can check for their presence using
the command \texttt{which}:
\begin{obeylines}
  \texttt{
    \%\% which make
    /usr/bin/make
    \%\% which gcc
    /usr/bin/gcc
    }
\end{obeylines}
\bigskip \noindent
If you don't have them, you will need to get them.

Assuming all is well, we issue the command \texttt{make} and see the text that the command directs to the screen:
\begin{obeylines}
  \texttt{
    \%\% make
    gcc -I. -Wall -Wextra  -c normal.c
    gcc -o make\_ex example.c normal.o -lm
    gcc -I. -Wall -Wextra  -c parser.c
    gcc -I. -Wall -Wextra  -c subset.c
    gcc -I. -Wall -Wextra  -c cee.c
    gcc -I. -Wall -Wextra  -c schchprob.c
    gcc -I. -Wall -Wextra  -c partalloc.c
    gcc -I. -Wall -Wextra  -c push\_relabel.c
    gcc -I. -Wall -Wextra  -c gcps\_solver.c
    gcc -o gcps solve.c normal.o parser.o subset.o cee.o
    \ \ schchprob.o partalloc.o push\_relabel.o gcps\_solver.o -lm
    gcc -I. -Wall -Wextra  -c implement.c
    gcc -o purify purify.c normal.o parser.o subset.o partalloc.o
    \ \ implement.o -lm
    } 
\end{obeylines}
\bigskip
\noindent
Each line of output above corresponds to one of the commands in the
\texttt{makefile}, and each of the commands in the makefile specifies
an object to be constructed (either an object (that is, a \texttt{.o}
file) or an executable) the resources that are required to construct
it, and the command that constructs it.  (There is also an object
\texttt{all}, which requires the three executables.  When make is
asked to construct something (e.g., the command \texttt{make gcps}) it
first makes all the resources that that thing requires, so
\texttt{make all} will result in constructing all of the executables
at once. Because \texttt{make}'s default behavior is to construct the
first object in the \texttt{makefile}, \texttt{make} has the same
effect as \texttt{make all}.  There is also an object \texttt{clean}.
The command \texttt{make clean} removes the objects and executables,
and also any of the \texttt{*\~} files that the \texttt{emacs} editor
leaves behind after a preexisting file has been edited. The command
\texttt{make clean} takes us back to the situation before
\texttt{make} was invoked, and after that you can issue the command
\texttt{make} again to see it all happen again.

There are now the executables \texttt{make\_ex}, \texttt{gcps}, and
\texttt{purify}.  On many Unix's these can invoked simply by typing
the executable name on the command line, but it may be the case that,
for security reasons, the current directory is not in the
\texttt{path} (the list of directories that the command line looks in
when a command is invoked) in which case you will need to type
\texttt{./make\_ex}, \texttt{./gcps}, and \texttt{./purify}.  We begin
with \texttt{make\_ex}:
\begin{obeylines}
  \texttt{
    \%\% make\_ex
    /* This file was generated by make\_ex with 2 schools, 3 students
    per school, capacity 4 for all schools, school valence standard
    deviation 1.00, and idiosyncratic standard deviation 1.00. */
There are 6 students and 2 schools
The vector of quotas is (4,4)
The priority matrix is
\ \ \     1    1
\ \ \     1    1
\ \ \     1    0
\ \ \     1    0
\ \ \     1    0
\ \ \     0    1
The students numbers of ranked schools are
(2,2,1,1,1,1)
The preferences of the students are
1:    1   2
2:    1   2
3:    1
4:    1
5:    1
6:    2
The priority thresholds of the schools are
1   1   
    }
\end{obeylines}
\bigskip
\noindent
\textbf{Warning:} If what you get looks a bit different, it may be
because your installation of C and mine have different random number
generators.

Let's redirect the output to the file \texttt{schools.scp}, then invoke \texttt{gcps}:
\begin{obeylines}
  \texttt{
    \%\% make\_ex > schools.scp
    \%\% gcps
  /* This is a sample introductory comment. */
There are 6 students and 2 schools
\ \ \ \ \ \ \ \ \           1:   \ \ \ \ \ \ \        2:
1:   0.50000000  0.50000000
2:   0.50000000  0.50000000
3:   1.00000000  0.00000000
4:   1.00000000  0.00000000
5:   1.00000000  0.00000000
6:   0.00000000  1.00000000
  }
\end{obeylines}
\bigskip



Finally, let's redirect the output of \texttt{gcps} to the file \texttt{allocate.mat}, then invoke \texttt{purify}:
\begin{obeylines}
  \texttt{
    \%\% gcps > allocate.mat
    \%\% purify
    /* This is a sample introductory comment. */
\ \ \ \          1:   2:
   1:    0 \ \    1
   2:    1 \ \    0
   3:    1 \ \    0
   4:    1 \ \    0
   5:    1 \ \    0
   6:    0 \ \    1
  }
\end{obeylines}
\bigskip

That's all there is to it! We've now been through a complete cycle,
and the rest is up to you.  If you feel like it, you may want to
experiment with different parameters for \texttt{make\_ex} by editing
the file \texttt{example.c}, as described in Section \ref{sec:MakeEx},
then running \texttt{make} again and going through the
\texttt{make\_ex}-\texttt{gcps}- \texttt{purify} cycle.  This will
give you an initial feel for how fast \texttt{gcps} is.  (It's
\textit{very} fast.)  But after reading the rest of this guide, you
may well have your own ideas concerning what to do next.

\section{About the Code} \label{app:Code}

As we have mentioned earlier, we hope that our code provides a useful
starting point for others, either contributing to the repository at
Github, or for applications to districts with particular features.
For this reason we have kept things as simple as possible, even if
that entails somewhat less convenience for the user.  In particular,
the input and output formats are inflexible, and some users will
probably want to develop more sophisticated interfaces.

In this Appendix we provide an overview of the code, passing from the
simpler and more basic files to increasingly higher levels, in each
case describing those features that might not be so obvious.  Our hope
is to ease the process of learning about the code by providing a level
of explanation in which the objects in the code are described in human
terms, and in relation to the earlier descriptions of the algorithms.
While reading the descriptions of the files below, the reader should
also be looking at the files themselves, and especially the header
(\texttt{*.h}) files.

Before diving into details, here are some general remarks.  First,
although we have used C rather than C++ (for a project as small as
this, the various advantages of C++ seem not worth the additional
complexity of that language) the code is object oriented in spirit,
being organized as interactions of objects given by \texttt{struct}s.
Most of the time objects are ``passed by reference'' to functions,
which means that instead of passing the object itself, what is passed
is a pointer to the object.  Understanding the pointer concept of C is
a prerequisite to any detailed understanding of the code.

With perhaps one or two exceptions, each object has a destroyer, which
frees the memory that stores the object's data, and for many objects
there is a way of printing the object.  These printing functions
provide the format of the output of \texttt{make\_ex}, \texttt{gcps},
and \texttt{purify}, and for other objects the printing functions can
be useful for debugging.  In all cases the code for these functions is
simple and straightforward, and printing and destroyer functions will
not be mentioned below.  When studying the code, the reader can mostly
ignore the many calls to destroyers, trusting that the allocation and
freeing of memory is being handled correctly.

In the C programming language, an $n$ element array is indexed by the
integers $0, \ldots, n-1$.  We always think of it as indexed by the
integers $1, \ldots, n$, so the $j^{\text{th}}$ component of
\texttt{vec} is \texttt{vec[j-1]}.  Similarly, the $(i,j)$ component
of a matrix \texttt{mat} is \texttt{mat[i-1][j-1]}.  While this is
perhaps not one of the most appealing features of C, and it certainly
adds bulk to the code, once you get used to it, in a curious way it
seems to enhance the readability of the code.

\subsection{\texttt{normal.h} and \texttt{normal.c}}

The function \texttt{min} computes the minimum of two doubles.  The
function \texttt{is\_integer} returns 1 (true) if the given double is
within one one millionth of an integer and 0 (false) otherwise.  In
general, throughout the code, two floating point numbers are regarded
as equal if they differ by less that one millionth.  This prevents
rounding error from creating a spurious impression that two numbers
differ.  Incidently, the reason that the numbers in the output of
\texttt{gcps} have many digits is that an output of \texttt{gcps} must
be an accurate input for \texttt{purify}, so \texttt{gcps} shouldn't
(for example) print 0.99 instead of 0.99999999. The functions
\texttt{uniform} and \texttt{normal} provided uniformly distributed
(in $[0,1]$) and normally distributed (for mean 0 and standard
deviation 1) pseudorandom numbers.

\subsection{\texttt{example.c}}

The file \texttt{example.c} contains the \texttt{main} function of
\texttt{make\_ex}, which contains all of the code that is involved in
generating an example.  Although the code is somewhat lengthy, the
process is a straight line:
\begin{enumerate}
  \item[(a)] Locate the schools and students around the circle.
  \item[(b)] Compute the matrix of distances between students and schools.
  \item[(c)] Generate normally distributed random valences for the schools.
  \item[(d)] The utility of student \texttt{i} for school \texttt{j}
    is the valence of \texttt{j} plus a normally distributed
    \texttt{(i,j)}-idiosyncratic shock minus the distance from
    \texttt{i} to \texttt{j}.
  \item[(e)] Each student's safe school is (roughly) the one that is closest.
  \item[(f)] Student \texttt{i}'s priority at school \texttt{j} is one
    if its utility for $i$ is not less than the utility of
    \texttt{i}'s safe school, and otherwise it is zero.
  \item[(g)] The preference of student \texttt{i} is the list of
    schools of priority one, in order of decreasing utility.
\end{enumerate}

\subsection{\texttt{parser.h} and \texttt{parser.c}}

Two parsing functions \texttt{sch\_ch\_prob\_from\_file} and
\texttt{allocation\_from\_file} are declared in \texttt{parser.h}.  As
their names suggest, these functions read data from files,
constructing, respectively, a school choice problem
(\texttt{sch\_ch\_prob}) and an allocation (\texttt{partial\_alloc}).
A valid input file has an opening comment, which begins with
\texttt{/*} and ends with \texttt{*/}, and a body.  In the body, in
addition to the usual white space characters (space, tab, and newline)
the characters `\texttt{(}', `\texttt{)}', and `\texttt{,}' are
treated as white space.  The body is divided into whitespace and
tokens, which are sequences of adjacent characters without any white
space that are preceeded and followed by white space.

Everything in \texttt{parser.c} is easy to understand.  There are
numerous functions checking that the verbal tokens are the ones that
are expected, and quitting with an error message if one of them
isn't. This makes the code extremely verbose and thoroughly
amateurish.  If the reader kindly refrains from looking in
\texttt{parser.c}, this author will be spared considerable
embarrassment.

\subsection{\texttt{subset.h} and \texttt{subset.c}}

One may represent a subset of $\{1, \ldots, n\}$ as an $n$-tuple of
0's and 1's, or as a list of its elements.  The first of these is
given by \texttt{subset}, which, in addition to the $n$-tuple
\texttt{indicator} of elements of $\{0,1\}$, keeps track of the number
of elements of the subset and the number of elements of the set it is
a subset of.  The second representation is given by \texttt{index}, in
which \texttt{no\_elements} is the number of elements of the subset
(not the containing set) and \texttt{indices} is a strictly increasing
\texttt{no\_elements}-tuple of elements of $\{1, \ldots,
\text{\texttt{large\_set\_size}}\}$.  The \texttt{index}
representation can be much more efficient when we are dealing with
little subsets of big sets.

The function
\texttt{index\_of\_subset} passes from the first to the second, and
\texttt{subset\_of\_index} goes in the other direction.  (Since an
\texttt{index} does not know the size of the set it is a subset of,
that piece of data is a required argument.) There is no index
representation of the empty set, and if \texttt{subset\_of\_index}
receives the empty set as an argument, it will complain and halt the
program.

A \texttt{index\_list} is a linked list of subsets in \texttt{index}
form.  

Mostly the functions in \texttt{subset.h} have self explanatory
titles, with code that is not hard to understand.  There may now be
some functions that are not used elsewhere, as I have not made an
effort to eliminate such functions when they may prove useful later,
and are illustrative of what is possible.

\subsection{\texttt{cee.h} and \texttt{cee.c}}

The most general notion of a \emph{communal endowment economy} (CEE)
has a finite set of \emph{agents}, a finite set of \emph{objects}, a
vector of (nonegative) \emph{requirements} for the agents, a vector of
(nonnegative) \emph{quotas} for the objects, and a matrix that
specifies, for each agent and each object, the maximum amount of the
object that the agent may consume.  A \emph{partial allocation} is
matrix that specifies a nonnegative consumption of each object by each
agent.  A partial allocation is \emph{feasible} if: a) no agent
consumes more than the allowed quantity of any object; b) the sum of
each agents allocation is her requirement; c) the sum of each object's
allocations does not exceed the object's quota.

In the context of school choice the agents are \emph{students} the
objects are \emph{schools}, and each student's requirement is one.
The most basic type 
As the project has progressed, the types of school choice CEE that are encompassed has expanded.  

\norev

A school choice \emph{communal endowment economy} (CEE) consists of
\texttt{no\_students} students, \texttt{no\_schools} schools, a
specification of \texttt{quotas} (i.e., capacities) for the schools,
and a matrix \texttt{priority} specifying a nonnegative integer
\texttt{priority[i-1][j-1]} for each student \texttt{i} and each
school \texttt{j}.  When a CEE occurs as a part of an input, the
\texttt{quotas} are usually integers, but partially allocated CEE's
are used in the computations, when the remaining unallocated
\texttt{quotas} are floating point numbers.  For this reason there are
\texttt{int\_cee}'s and \texttt{double\_cee}'s.  Some of the more
advanced functions in \texttt{cee.h} are specific to priorities that
are either $0$ or $1$; in Section \ref{sec:Priorities} we explained
how to pass from more complicated priorities to binary priorities
using priority thresholds.

The function
\texttt{sub\_double\_cee} computes the \texttt{sub\_cee} obtained from
\texttt{given\_cee} by restricting to the set of students given by
\texttt{stu\_index} and the set of schools given by
\texttt{sch\_index}.

\subsection{\texttt{schchprob.h} and \texttt{schchprob.c}}

A \emph{school choice problem} combines a CEE, which may be thought of
as describing the outcomes that are physically possible, with
preferences for the students and priority thresholds for the schools.
A student is \emph{eligible} for a school if her priority at that
school is at or above the school's priority threshold. A student's
(strict) preference is the list of the schools she is eligible for,
going from best to worst.  For convenience we keep track of each
student's number of eligible schools.

The underlying CEE may be either an \texttt{int\_cee} or a
\texttt{double\_cee}.  Typically the input school choice problem has
an \texttt{int\_cee}, and \texttt{double\_cee}'s are used in computing
an allocation, so there are \texttt{input\_sch\_ch\_prob}'s, which
have \texttt{int\_cee}'s, and \texttt{sch\_ch\_prob}'s, which have
\texttt{double\_cee}'s.  A \texttt{sch\_ch\_prob} is typically what
remains to be allocated after a certain time, so it has a member
\texttt{time\_remaining}.

The function \texttt{sch\_ch\_prob\_from\_input} takes an
\texttt{input\_sch\_ch\_prob} as input and passes to a
\texttt{sch\_ch\_prob} by converting the quotas from integers to
floating point numbers, and by setting \texttt{time\_remaining} to
1.0.  The function \texttt{reduced\_sch\_ch\_prob} passes from a
\texttt{sch\_ch\_prob} with general priorities and priority thresholds
to one in which the priorities of student \texttt{i} at school
\texttt{j} is one if \texttt{i} is eligible to attend school
\texttt{j} and zero otherwise, and the priority thresholds of all
schools are one, as described in Section \ref{sec:Priorities}.

During the allocation process, when a GMC inequality for a set $P$ of
schools is encountered, there is a smaller allocation problem for $P$
and the set $J_P$ of students who, at that point in the process, are
not eligible for any schools outside of $P$.  There is a similar
allocation problem for the complements $P^c$ and $J_P^c$ of $P$ and
$J_P$, and the continuation of the allocation process is the sum of
the allocation processes for these subproblems.  The function
\texttt{sub\_sch\_ch\_prob} constructs the subproblem for $J_P =
\text{\texttt{stu\_subset}}$ and $P = \text{\texttt{sch\_subset}}$ and
the subproblem for $J_P^c = \text{\texttt{stu\_compl}}$ and $P^c =
\text{\texttt{sch\_compl}}$.  

The function \texttt{time\_re\_after\_first\_gmc\_eq} computes the time
remaining if the allocation process continues until the GMC inequality
for \texttt{school\_subset} and \texttt{captive\_students} holds with
equality or the unit interval of time is exhausted, ignoring all other
constraints.  It may happen that the GMC inequality for these sets has
already been violated.  In this case the argument
\texttt{overallocated\_schools} is set equal to
\texttt{school\_subset}, and the current attempt at computing an
allocation is abandoned as quickly as possible, then restarted after
the list of schools being monitored has been adjusted by adding
\texttt{overallocated\_schools} to it.

The function \texttt{time\_remaining\_after\_first\_gmc\_eq} considers
all the school subsets in \texttt{known\_facets} and
\texttt{observed\_overallocated\_sets}, and all \texttt{related}
connected subsets with \texttt{depth} or few elements.  For each such
set of schools \texttt{time\_remaining\_of\_gmc\_eq} is applied to
that set and the set of students who cannot be assigned further
probability in schools outside that set.  It returns the maximum of
these quantities while setting the pointees of
\texttt{crit\_stu\_subset} and \texttt{crit\_sch\_subset} to subsets
that attain the maximum.

\subsection{\texttt{partalloc.h} and \texttt{partalloc.c}}

In a \texttt{partial\_alloc} for \texttt{no\_students} students and
\texttt{no\_schools} schools, \texttt{allocations} is a matrix that
specifies an allocation \texttt{allocations[i-1][j-1]} of school
\texttt{j} to student \texttt{i} for each \texttt{i} and \texttt{j}.
A \texttt{pure\_alloc} has the same structure, but now
\texttt{allocations[i-1][j-1]} is an integer that should be zero or
one, and for each student \texttt{i} there should be exactly one
school \texttt{j} such that \texttt{allocations[i-1][j-1]} is one.

The function
\texttt{allocate\_until\_new\_time} creates a \texttt{partial\_alloc}
in which each student receives
$$\text{\texttt{my\_scp->time\_remaining}} -
\text{\texttt{new\_time\_remaining}}$$ units of her favorite school
and none of any other school.  Increasing a \texttt{base} partial
allocation by adding an \texttt{increment} partial allocation to it is
what \texttt{increment\_partial\_alloc} does.  The function
\texttt{school\_sums} returns an array of \texttt{double} that
specifies, for each school, the total amount of it that has been
allocated in \texttt{my\_alloc}.

\subsection{\texttt{solver.h} and \texttt{solver.c}} \label{subsec:Solver}

The files \texttt{solver.h} and \texttt{solver.c} contain the top
level of code that defines the executable \texttt{gcps}.  The function
\texttt{GCPS\_schools\_solver\_top\_level} repeatedly attempts to find
the desired allocation by initiating a computation in which
\texttt{GCPS\_schools\_solver} calls itself recursively.

The subset list \texttt{known\_facets} is a list of sets of schools
that have been critical during some attempt.  An attempt can fail in two
ways.  First, it can encounter a critical set
\texttt{new\_critical\_set} of schools that has not been observed
before, in which case the \texttt{index} of
\texttt{new\_critical\_set} is added to \texttt{known\_facets} and the
next attempt begins.  The other possibility is that at some point the
computation discovers that a set \texttt{overallocated\_schools} of
schools has allocated its seats to an extent such that it can no
longer meet the needs of the students who cannot attend schools
outside the set.  The \texttt{index} of
\texttt{overallocated\_schools} is added to the list
\texttt{observed\_overallocated\_sets}, and another attempt is
initiated.

Observing an \texttt{overallocated\_schools} means that the allocation
process went past some GMC inequality, and it is necessary to find this
inequality before the process can succeed.  The integer parameter
\texttt{depth} is the maximal size of sets (in addition to those in
\texttt{known\_facets} and \texttt{observed\_overallocated\_sets})
that are examined when looking for either the next
\texttt{new\_critical\_set} or an \texttt{overallocated\_set}, which
may or may not be one that has already been seen.  Each time an
attempt returns an \texttt{overallocated\_set} that has been seen
before, \texttt{depth} is increased by one.  When an attempt returns a
\texttt{new\_critical\_set}, \texttt{depth} is reset to $1$.

Searching over sets of size \texttt{depth} throughout the entire
search process can be extremely burdensome, and it is natural to
expect that the missing \texttt{new\_critical\_set} can be found near
the end of the allocation process as it currently stands.  The
variable \texttt{level} is the depth of recursion, i.e., the number of
times \texttt{GCPS\_schools\_solver} has called itself. The maximum
set size searched over is set to \texttt{depth} only when
$$\text{\texttt{max\_level}} - \text{\texttt{depth}} \le
\text{\texttt{level}} \le \text{\texttt{max\_level}},$$ where
\texttt{max\_level} is the \texttt{level} at which the
\texttt{overallocated\_set} is found.

We now turn to the description of \texttt{GCPS\_schools\_solver}. The
key function applied here is
\texttt{time\_rem\_after\_first\_gmc\_eq}, which is
coded in \texttt{schchprob.h} and \texttt{schchprob.h}. The main
purpose of this function is to compute the amount of time that will
remain if the allocation continues allocating the favorite school to
each student until there is a critical set of schools or an
overallocated set of schools, among the sets of schools that are
searched over.  A side effect of this function is to compute the
critical or overallocated set.  If the critical set has not been
observed before, or if there is an overallocated set, the process
reports this back to the instance of \texttt{GCPS\_schools\_solver}
that called it, or to \texttt{GCPS\_schools\_solver\_top\_level}, by
returning a partial allocation that will be discarded.

Otherwise \texttt{GCPS\_schools\_solver} allocates up until the
computed time, and it then splits the remaining allocation problem
into two subproblems, and calls itself on each of these. The first
``left'' subproblem relates to the critical set \texttt{sch\_subset}
and the set \texttt{stu\_subset} of students who are no longer able to
consume any school outside of this set.  The second ``right''
subproblem is obtained by restricting to the complements
\texttt{stu\_compl} and \texttt{sch\_compl} of these sets.  For each
of these subproblems the result may be either a
\texttt{new\_critical\_set} or an \texttt{overallocated\_set}, or it
may be a complete allocation for the subproblem.

When we look at the coding of
\texttt{time\_rem\_after\_first\_gmc\_eq} in
\texttt{schchprob.c}, we see that it scans over a collection of sets
of schools. For each set it asks how long the allocation process could
allocate the favorite school to each student before that set becomes
critical.  If the amount of time is negative, then the set is
overallocated.  If it finds an overallocated set of schools, it
reports that set and sets \texttt{exit\_status} to $2$.  If there is
no overalllocated school, then it reports the maximum time remaining
after some set becomes critical, and a set of schools that attains
this maximum, setting \texttt{exit\_status} to $1$ if that set's
\texttt{index} is not already in \texttt{known\_facets}.

The collection of sets that \texttt{time\_rem\_after\_first\_gmc\_eq}
scans over consists of the sets in \texttt{known\_facets} and
\texttt{observed\_overallocated\_sets}, and the sets generated by the
function \texttt{next\_subset}, which is coded in \texttt{subset.h}
and \texttt{subset.c}.  This function depends of the matrix
\texttt{related}, which encodes the undirected graph whose nodes are
the schools that still might exhaust their capacities, with an edge
between schools \texttt{i} and \texttt{j} if there is a student who
might attend either one.  A minimal critical set necessarily induces a
connected subgraph of \texttt{related}.  One could enumerate all the
sets of schools of size \texttt{subset\_size} with this properties by
first enumerating all the sets of schools that might still fill up of
size \texttt{subset\_size}, then asking for each one whether it
induces a connected subgraph of \texttt{related}, but when the set of
schools is large, this is very laborious.  Instead
\texttt{next\_subset} takes advantage of the fact that each set of
schools that induces a connected subgraph has a unique ordering in
which the first school is the least (according to the numbering of the
schools) school in the set, the second school is the least among the
remainder that is connected to the first, the third is the least in
the remainder that is connected to either the first or the second, and
so forth.  These orderings of schools can be ordered
lexicographically, and \texttt{next\_subset} passes from a set to the
next set in this ordering.  The code that does this is intricate, and
perhaps the most difficult part of the code to understand.

\subsection{\texttt{implement.h} and \texttt{implement.c}}

The code of the algorithm going from a fractional allocation to a
random pure allocation whose distribution has the given allocation as
its average follows the description in Section
\ref{sec:Implementation}.  The \texttt{nonintegral\_graph} derived
from the given allocation is an undirected graph with an edge between
a student and a school if the student's allocation of the school is
strictly between zero and one, and an edge between a school and the
sink if the total allocation of the school is not an integer.  The
function \texttt{graph\_from\_alloc} has the given allocation as its
input, and its output is the derived \texttt{nonintegral\_graph}.

Especially for large school choice problems, we expect the
\texttt{nonintegral\_graph} to be quite sparse, so it can be
represented more compactly, and be easier to work with, if we encode
it by listing the neighbors of each node.  The \texttt{stu\_sch\_nbrs}
member of \texttt{neighbor\_lists} is a list of \texttt{no\_students}
lists, where the \texttt{stu\_sch\_nbrs[i-1]} are arrays of varying
dimension. We set \texttt{stu\_sch\_nbrs[i-1][0] = 0} in order to have
a place holder that allows us to not have an array with no entries
when \texttt{i} has no neighbors.  The actual neighbors of \texttt{i}
are
$$\text{\texttt{stu\_sch\_nbrs[i-1][1],...,stu\_sch\_nbrs[i-1][stu\_no\_nbrs[i-1]]}}.$$
The members \texttt{sch\_no\_nbrs} and \texttt{sink\_sch\_nbrs} follow
this pattern, except that in the latter case there is just a single
list.  The member \texttt{sch\_sink\_nbrs} is a
\texttt{no\_schools}-dimensional array of integers with
\texttt{sch\_sink\_nbrs[j-1] = 1} if there is an edge connecting
\texttt{j} and the \texttt{sink} and \texttt{sch\_sink\_nbrs[j-1] = 0}
otherwise.  To pass from a \texttt{nonintegral\_graph} to its
representation as a \texttt{neighbor\_lists} we apply
\texttt{neighbor\_lists\_from\_graph}.

A cycle in the \texttt{nonintegral\_graph} is a linked list of
\texttt{path\_node}'s.  The function \texttt{find\_cyclic\_path}
implements the algorithm for finding a cycle that we described in
Section \ref{sec:Implementation}.  Given a cycle,
\texttt{bound\_of\_cycle} computes the smallest ``alternating
perturbation,'' in one direction or the other, of the entries of (the
pointee of) \texttt{my\_alloc} that turns some component of the
allocation, or some total allocation of a school, into an integer.
For such an \texttt{adjustment} the function
\texttt{cyclic\_adjustment} updates the allocation, and it calls the
functions \texttt{student\_edge\_removal} and
\texttt{sink\_edge\_removal} to update \texttt{neighbor\_lists}.
When \texttt{graph\_is\_nonempty(my\_lists) = 0} (false) the entries
of \texttt{my\_alloc} are doubles that are all very close to integers,
and the function \texttt{pure\_allocation\_from\_partial} passes to
the associated \texttt{pure\_alloc}.  The function
\texttt{random\_pure\_allocation} is the master function that
supervises the whole process.

\subsection{\texttt{solve.c} and  \texttt{purify.c}}

The files \texttt{solve.c} and \texttt{purify.c} contain the
\texttt{main} functions of the executables \texttt{gcps} and
\texttt{purify} respectively.  These \texttt{main} functions are
simple and straightforward.

\end{appendix}

\end{document}

